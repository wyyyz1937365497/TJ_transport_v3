# 数据加载优化策略

## 一、硬件配置概述

| 组件 | 配置 | 关键约束 |
|------|------|----------|
| GPU | 2块NVIDIA RTX 2080 Ti（每张22GB显存） | 总显存44GB，支持双卡训练 |
| CPU | 32线程 | 可支持高并发数据加载 |
| 系统内存 | 16GB | **严格约束**，需控制在12-14GB以内 |
| 环境 | WSL2 | 跨文件系统I/O性能需优化 |

## 二、内存优化策略

### 2.1 核心原则
- **避免一次性加载全部数据**：使用流式加载或分块加载
- **内存映射文件技术**：对于大型数据集使用mmap
- **及时释放不再使用的数据**：在训练循环中主动清理
- **控制峰值内存**：目标控制在12-14GB以内

### 2.2 分层内存管理

#### 基础层：数据存储优化
```
数据文件组织结构：
/data/
  ├── phase1/              # Phase 1预训练数据
  │   ├── chunks/          # 分块存储（每块100-200样本）
  │   │   ├── chunk_000.json
  │   │   ├── chunk_001.json
  │   │   └── ...
  │   ├── metadata.json    # 元数据索引
  │   └── statistics.json  # 数据统计信息
  ├── phase2/              # Phase 2 RL训练数据
  │   └── ...
  └── phase3/              # Phase 3约束优化数据
      └── ...
```

#### 实施策略

**策略1：分块数据加载**
- 将1000个样本分成10-20个chunks（每块50-100样本）
- 使用`ChunkedDataset`类，按需加载chunks
- 实现LRU缓存机制，保留最近使用的2-3个chunks在内存
- 预期内存占用：2-3个chunks × 100样本 × 约50KB/样本 ≈ 10-15MB

**策略2：内存映射文件（Memory-Mapped Files）**
- 对于大型JSON文件，使用`mmap`模块或`numpy.memmap`
- 将图数据（节点特征、边特征）转换为二进制格式（.npy）
- 优势：操作系统自动管理页面置换，减少内存压力
- 实施步骤：
  1. 预处理阶段：将JSON转换为HDF5或NPY格式
  2. 训练时：使用`h5py`或`numpy.load(..., mmap_mode='r')`
  3. 按需读取特定样本，避免全量加载

**策略3：数据流式加载**
- 实现`StreamingDataset`，支持逐样本或逐batch加载
- 使用生成器模式，避免在内存中存储完整数据集
- 适用于Phase 2和Phase 3的在线训练场景

**策略4：主动内存释放**
```python
# 在训练循环中定期清理
def train_epoch_with_memory_cleanup():
    for batch_idx, batch in enumerate(dataloader):
        # 训练逻辑
        loss = train_step(batch)
        
        # 每10个batch清理一次缓存
        if batch_idx % 10 == 0:
            torch.cuda.empty_cache()
            gc.collect()
```

### 2.3 内存监控与预警

**监控指标：**
- 峰值内存使用量
- DataLoader缓存大小
- GPU显存使用情况
- 系统可用内存

**预警机制：**
- 当内存使用超过12GB时，触发警告
- 当内存使用超过14GB时，自动触发清理
- 记录内存使用曲线，用于后续优化

## 三、DataLoader优化策略

### 3.1 核心配置参数

基于32线程CPU和16GB内存的约束，推荐配置：

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| **num_workers** | 4-6 | 不超过CPU核心数/2，避免过度竞争 |
| **pin_memory** | True | 加速CPU到GPU数据传输 |
| **prefetch_factor** | 2 | 预取2个batch，平衡内存和性能 |
| **batch_size** | 8-16 | 根据显存和内存动态调整 |
| **persistent_workers** | True | 避免worker重复创建开销 |
| **drop_last** | True | 避免最后一个不完整batch |

### 3.2 阶段性配置

#### Phase 1：世界模型预训练（静态数据）
```python
dataloader_config = {
    'num_workers': 4,           # 保守配置，避免内存压力
    'pin_memory': True,
    'prefetch_factor': 2,
    'batch_size': 16,           # 较大batch，充分利用GPU
    'persistent_workers': True,
    'drop_last': True,
    'shuffle': True
}
```

**内存估算：**
- Batch大小：16样本
- 每样本图数据：约50KB（节点+边+索引）
- Batch内存：16 × 50KB = 800KB
- Worker缓存：4 workers × 2 prefetch × 800KB ≈ 6.4MB
- 总DataLoader内存：< 10MB

#### Phase 2：安全RL训练（SUMO实时交互）
```python
dataloader_config = {
    'num_workers': 2,           # 减少worker，因为SUMO环境占用资源
    'pin_memory': True,
    'prefetch_factor': 2,
    'batch_size': 8,            # 较小batch，适应实时环境
    'persistent_workers': True,
    'drop_last': False,
    'shuffle': False            # 顺序处理环境观测
}
```

**内存估算：**
- Batch内存：8 × 50KB = 400KB
- Worker缓存：2 × 2 × 400KB ≈ 1.6MB
- SUMO环境内存：每个环境约200-500MB
- 总内存：环境内存 + DataLoader缓存

#### Phase 3：约束优化（静态数据）
```python
dataloader_config = {
    'num_workers': 6,           # 增加worker，充分利用CPU
    'pin_memory': True,
    'prefetch_factor': 2,
    'batch_size': 12,           # 中等batch
    'persistent_workers': True,
    'drop_last': True,
    'shuffle': True
}
```

### 3.3 DataLoader优化技巧

**技巧1：自定义Collate函数**
- 优化batch构建过程，减少内存拷贝
- 使用`torch.stack`替代列表拼接
- 实现稀疏图数据的批量处理

**技巧2：动态Batch Size**
```python
# 根据可用内存动态调整batch_size
def get_dynamic_batch_size():
    available_memory = get_available_memory_gb()
    if available_memory > 4:
        return 16
    elif available_memory > 2:
        return 8
    else:
        return 4
```

**技巧3：使用IterableDataset**
- 对于流式数据或SUMO实时数据
- 避免一次性加载所有数据
- 支持无限数据生成

## 四、数据预处理优化策略

### 4.1 预处理和缓存策略

#### 策略1：离线预处理
- 在训练前完成所有数据预处理
- 将预处理结果保存到磁盘
- 训练时直接加载预处理数据

**预处理流程：**
```
原始JSON数据
    ↓
[1] 解析JSON，提取车辆数据
    ↓
[2] 构建图结构（节点、边、特征）
    ↓
[3] 特征归一化
    ↓
[4] 数据增强（可选）
    ↓
[5] 保存为二进制格式（HDF5/NPY）
    ↓
预处理完成，可直接用于训练
```

#### 策略2：智能缓存
- 使用`functools.lru_cache`缓存计算结果
- 实现多级缓存：
  - L1缓存：内存中的最近计算结果（100-200条）
  - L2缓存：磁盘上的预处理数据
  - L3缓存：原始数据文件

**缓存配置：**
```python
# L1缓存：GNN嵌入缓存
gnn_cache = LRUCache(maxsize=200, timeout=10)  # 缓存200个嵌入，10秒过期

# L2缓存：预处理图数据
graph_cache = DiskCache(cache_dir='./cache/graph_cache', max_size_gb=2)

# L3缓存：原始数据
# 直接从文件读取，不缓存
```

### 4.2 图数据预处理优化

#### 优化1：边索引稀疏化
- 使用稀疏矩阵存储边索引（COO格式）
- 减少内存占用，提高计算效率
- 适用于大规模图数据

#### 优化2：特征量化
- 将浮点特征量化为低精度格式（float16或int8）
- 节省内存，加速计算
- 在训练时动态解码

**量化方案：**
```
原始：float32 (4 bytes/feature)
量化后：float16 (2 bytes/feature)
节省：50%内存

节点特征：N × 9 × 4 bytes → N × 9 × 2 bytes
边特征：E × 4 × 4 bytes → E × 4 × 2 bytes
```

#### 优化3：图数据分片
- 将大图拆分为多个子图
- 按区域或时间分片
- 支持分布式训练

### 4.3 特征归一化预处理

#### 归一化策略
- **位置特征**：Min-Max归一化到[0, 1]
- **速度特征**：Z-score标准化（均值0，标准差1）
- **加速度特征**：Clamp到[-5, 5]后归一化
- **时间特征**：正弦/余弦编码（周期性）

**归一化参数缓存：**
```python
normalization_params = {
    'position': {'min': 0.0, 'max': 1000.0},
    'speed': {'mean': 15.0, 'std': 5.0},
    'acceleration': {'min': -5.0, 'max': 5.0},
    # ... 其他特征
}
```

### 4.4 数据增强策略

#### 增强方法
1. **时间平移**：随机偏移时间步
2. **速度扰动**：添加高斯噪声（σ=0.1）
3. **位置抖动**：轻微位置偏移（±5米）
4. **边随机丢弃**：随机丢弃10%的边
5. **节点特征掩码**：随机遮蔽5%的节点特征

**增强配置：**
```python
augmentation_config = {
    'time_shift': {'enabled': True, 'range': (-10, 10)},
    'speed_noise': {'enabled': True, 'std': 0.1},
    'position_jitter': {'enabled': True, 'range': (-5, 5)},
    'edge_dropout': {'enabled': True, 'rate': 0.1},
    'node_mask': {'enabled': True, 'rate': 0.05}
}
```

**增强策略：**
- Phase 1：启用所有增强（提高模型泛化）
- Phase 2：禁用增强（使用真实环境数据）
- Phase 3：启用部分增强（时间平移、速度扰动）

## 五、SUMO环境数据加载优化

### 5.1 并行环境数量优化

#### 推荐配置
| 场景 | 并行环境数 | 说明 |
|------|-----------|------|
| 单GPU训练 | 2-3个环境 | 平衡速度和内存 |
| 双GPU训练 | 4-6个环境 | 每GPU 2-3个环境 |
| CPU密集型 | 1-2个环境 | 避免CPU过载 |

**内存估算：**
- 单个SUMO环境：约200-500MB
- 2个环境：400MB-1GB
- 4个环境：800MB-2GB
- 6个环境：1.2GB-3GB

**配置建议：**
```python
# 单GPU配置
num_envs = 2  # 保守配置
# 双GPU配置
num_envs = 4  # 每GPU 2个环境
```

### 5.2 观测数据缓存

#### 缓存策略
- **TraCI订阅缓存**：已实现，缓存超时0.1秒
- **观测历史缓存**：缓存最近T步的观测（T=10-20）
- **图数据缓存**：缓存最近构建的图结构

**缓存实现：**
```python
class ObservationCache:
    def __init__(self, max_size=20):
        self.cache = OrderedDict()
        self.max_size = max_size
    
    def get(self, step):
        if step in self.cache:
            self.cache.move_to_end(step)
            return self.cache[step]
        return None
    
    def put(self, step, observation):
        self.cache[step] = observation
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)
```

### 5.3 异步数据收集

#### 异步架构
```
主训练线程
    ↓
[异步数据收集线程]
    ↓
SUMO环境池（2-4个环境）
    ↓
观测队列（Buffer Size=100）
    ↓
DataLoader（批量处理）
    ↓
训练循环
```

**实现要点：**
- 使用`multiprocessing.Queue`或`torch.multiprocessing`
- 异步线程持续收集观测
- 主线程从队列获取batch
- 队列满时阻塞，避免内存溢出

**配置参数：**
```python
async_config = {
    'num_collectors': 2,           # 异步收集线程数
    'buffer_size': 100,            # 观测队列大小
    'prefetch_batches': 2,         # 预取batch数
    'timeout': 5.0                 # 超时时间（秒）
}
```

### 5.4 内存管理策略

#### 策略1：环境复用
- 不频繁创建/销毁SUMO环境
- 使用`reset()`方法重置环境
- 减少内存碎片

#### 策略2：梯度累积
- 对于小batch场景，使用梯度累积
- 累积多个小batch后再更新
- 减少内存占用

**梯度累积配置：**
```python
accumulation_steps = 4  # 累积4个batch
for i, batch in enumerate(dataloader):
    loss = train_step(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

#### 策略3：混合精度训练
- 使用`torch.cuda.amp`自动混合精度
- 减少显存占用约50%
- 加速训练约1.5-2倍

**实现：**
```python
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    output = model(batch)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## 六、分布式数据加载策略

### 6.1 DistributedSampler配置

#### 基础配置
```python
from torch.utils.data.distributed import DistributedSampler

sampler = DistributedSampler(
    dataset=dataset,
    num_replicas=world_size,        # 2（双GPU）
    rank=rank,                      # 0或1
    shuffle=True,                   # 每个epoch重新shuffle
    drop_last=True,                 # 丢弃最后一个不完整batch
    seed=42                         # 固定随机种子
)
```

#### 关键参数说明
- **num_replicas**：进程数（2个GPU=2）
- **rank**：当前进程ID（0或1）
- **shuffle**：每个epoch开始时重新shuffle
- **drop_last**：确保每个进程batch数相同

### 6.2 数据分片策略

#### 策略1：均匀分片
- 将数据集均匀分配到各GPU
- 每个GPU处理50%的数据
- 适用于数据量适中的场景

**分片示例：**
```
总数据：1000样本
GPU 0：样本0-499（500样本）
GPU 1：样本500-999（500样本）
```

#### 策略2：动态分片
- 根据GPU性能动态调整分片
- 快速GPU处理更多数据
- 适用于异构GPU场景

**实现：**
```python
# 根据GPU性能计算分片比例
gpu_performance = {
    0: 1.0,   # 基准性能
    1: 1.0    # 相同性能
}

total_ratio = sum(gpu_performance.values())
split_ratios = {k: v/total_ratio for k, v in gpu_performance.items()}
```

### 6.3 负载均衡优化

#### 优化1：预取平衡
- 每个GPU独立预取数据
- 避免GPU间数据竞争
- 使用`DistributedDataParallel`的自动平衡

#### 优化2：梯度同步优化
- 使用`torch.distributed`的异步梯度同步
- 减少同步等待时间
- 配置`bucket_cap_mb`参数

**配置：**
```python
torch.distributed.init_process_group(
    backend='nccl',                # 使用NCCL后端（GPU）
    init_method='env://',
    world_size=world_size,
    rank=rank
)

model = torch.nn.parallel.DistributedDataParallel(
    model,
    device_ids=[local_rank],
    bucket_cap_mb=25,              # 梯度桶大小
    find_unused_parameters=False
)
```

### 6.4 数据重复避免

#### 策略1：确定性采样
- 使用固定随机种子
- 确保每个epoch的采样顺序可复现
- 便于调试和对比实验

#### 策略2：去重机制
- 在数据预处理阶段去重
- 使用哈希值标识唯一样本
- 避免重复训练相同数据

## 七、WSL2特定优化策略

### 7.1 文件I/O性能优化

#### 优化1：使用WSL2文件系统
- 将数据文件放在WSL2文件系统中（如`/home/wyyyz/TJ_transport_v3/data`）
- 避免跨文件系统访问（如访问Windows的`C:\`盘）
- WSL2文件系统性能显著优于跨文件系统访问

**性能对比：**
```
WSL2文件系统：~500 MB/s
跨文件系统（/mnt/c/）：~50-100 MB/s
性能提升：5-10倍
```

#### 优化2：使用tmpfs（内存文件系统）
- 将频繁访问的临时数据放在`/dev/shm`
- 完全在内存中，无磁盘I/O
- 适用于小型缓存文件

**使用示例：**
```bash
# 创建tmpfs挂载点
sudo mount -t tmpfs -o size=2G tmpfs /dev/shm/cache

# 使用tmpfs存储缓存
cache_dir = '/dev/shm/cache'
```

### 7.2 跨文件系统访问优化

#### 策略1：数据本地化
- 将训练数据复制到WSL2文件系统
- 避免每次训练都从Windows盘读取
- 节省大量I/O时间

**实施步骤：**
```bash
# 复制数据到WSL2文件系统
cp -r /mnt/c/path/to/data /home/wyyyz/TJ_transport_v3/data

# 训练时使用本地数据
data_path = '/home/wyyyz/TJ_transport_v3/data'
```

#### 策略2：符号链接
- 如果必须访问Windows文件
- 创建符号链接到WSL2文件系统
- 减少路径解析开销

```bash
# 创建符号链接
ln -s /mnt/c/path/to/data /home/wyyyz/TJ_transport_v3/data_link
```

### 7.3 内存映射优化

#### 优化1：使用mmap_mode='r'
- 对于大型数据文件，使用只读内存映射
- 操作系统自动管理页面置换
- 减少内存占用

**实现：**
```python
import numpy as np

# 使用内存映射加载
node_features = np.load('node_features.npy', mmap_mode='r')
edge_features = np.load('edge_features.npy', mmap_mode='r')
edge_indices = np.load('edge_indices.npy', mmap_mode='r')

# 按需读取特定样本
sample_features = node_features[100:200]  # 只加载这部分到内存
```

#### 优化2：使用HDF5格式
- HDF5支持高效的分块存储和内存映射
- 适合大规模图数据
- 支持压缩，节省磁盘空间

**HDF5优势：**
```
- 支持分块存储（chunked storage）
- 支持压缩（gzip, lzf, szip）
- 支持内存映射（mmap）
- 支持并行I/O
```

**配置示例：**
```python
import h5py

# 创建HDF5文件
with h5py.File('data.h5', 'w') as f:
    f.create_dataset('node_features', 
                     data=node_features,
                     chunks=(100, 9),       # 分块大小
                     compression='gzip',    # 压缩
                     compression_opts=9)    # 压缩级别

# 读取时使用内存映射
with h5py.File('data.h5', 'r') as f:
    dset = f['node_features']
    # 按需读取
    sample = dset[100:200]
```

## 八、综合配置建议

### 8.1 Phase 1配置（世界模型预训练）

```python
phase1_config = {
    # DataLoader配置
    'dataloader': {
        'num_workers': 4,
        'pin_memory': True,
        'prefetch_factor': 2,
        'batch_size': 16,
        'persistent_workers': True,
        'drop_last': True,
        'shuffle': True
    },
    
    # 内存管理
    'memory': {
        'use_mmap': True,
        'chunk_size': 100,
        'cache_chunks': 2,
        'cleanup_interval': 10,
        'target_memory_gb': 12
    },
    
    # 数据预处理
    'preprocessing': {
        'offline': True,
        'cache_dir': './cache/phase1',
        'normalization': True,
        'augmentation': True
    },
    
    # 分布式配置
    'distributed': {
        'enabled': True,
        'backend': 'nccl',
        'sampler_shuffle': True,
        'gradient_accumulation': 1
    },
    
    # WSL2优化
    'wsl2': {
        'use_tmpfs': False,
        'data_localized': True,
        'avoid_cross_fs': True
    }
}
```

**预期效果：**
- 内存占用：10-12GB
- 训练速度：提升30-50%
- GPU利用率：80-90%

### 8.2 Phase 2配置（安全RL训练）

```python
phase2_config = {
    # DataLoader配置
    'dataloader': {
        'num_workers': 2,
        'pin_memory': True,
        'prefetch_factor': 2,
        'batch_size': 8,
        'persistent_workers': True,
        'drop_last': False,
        'shuffle': False
    },
    
    # SUMO环境配置
    'sumo': {
        'num_envs': 2,
        'async_collection': True,
        'buffer_size': 100,
        'prefetch_batches': 2,
        'use_subscription': True,
        'cache_timeout': 0.1
    },
    
    # 内存管理
    'memory': {
        'use_mmap': False,
        'cleanup_interval': 5,
        'target_memory_gb': 10,
        'mixed_precision': True,
        'gradient_accumulation': 4
    },
    
    # 数据预处理
    'preprocessing': {
        'offline': False,
        'cache_dir': './cache/phase2',
        'normalization': True,
        'augmentation': False
    },
    
    # 分布式配置
    'distributed': {
        'enabled': True,
        'backend': 'nccl',
        'sampler_shuffle': False,
        'gradient_accumulation': 4
    },
    
    # WSL2优化
    'wsl2': {
        'use_tmpfs': True,
        'tmpfs_size': '1G',
        'data_localized': True,
        'avoid_cross_fs': True
    }
}
```

**预期效果：**
- 内存占用：8-10GB
- 环境吞吐量：提升20-30%
- GPU利用率：70-80%

### 8.3 Phase 3配置（约束优化）

```python
phase3_config = {
    # DataLoader配置
    'dataloader': {
        'num_workers': 6,
        'pin_memory': True,
        'prefetch_factor': 2,
        'batch_size': 12,
        'persistent_workers': True,
        'drop_last': True,
        'shuffle': True
    },
    
    # 内存管理
    'memory': {
        'use_mmap': True,
        'chunk_size': 100,
        'cache_chunks': 3,
        'cleanup_interval': 10,
        'target_memory_gb': 13,
        'mixed_precision': True
    },
    
    # 数据预处理
    'preprocessing': {
        'offline': True,
        'cache_dir': './cache/phase3',
        'normalization': True,
        'augmentation': True,
        'augmentation_config': {
            'time_shift': True,
            'speed_noise': True,
            'edge_dropout': False,
            'node_mask': False
        }
    },
    
    # 分布式配置
    'distributed': {
        'enabled': True,
        'backend': 'nccl',
        'sampler_shuffle': True,
        'gradient_accumulation': 1
    },
    
    # WSL2优化
    'wsl2': {
        'use_tmpfs': False,
        'data_localized': True,
        'avoid_cross_fs': True
    }
}
```

**预期效果：**
- 内存占用：11-13GB
- 训练速度：提升40-60%
- GPU利用率：85-95%

## 九、实施步骤

### 9.1 数据预处理阶段

1. **数据格式转换**
   - 将JSON数据转换为HDF5/NPY格式
   - 实现图数据的分块存储
   - 计算并保存归一化参数

2. **数据增强应用**
   - 对Phase 1和Phase 3数据应用增强
   - 保存增强后的数据
   - 生成数据索引文件

3. **元数据生成**
   - 生成数据统计信息
   - 创建数据分片索引
   - 保存数据分布信息

### 9.2 训练配置阶段

1. **创建配置文件**
   - 为每个阶段创建独立配置
   - 设置DataLoader参数
   - 配置内存管理策略

2. **环境准备**
   - 创建必要的目录结构
   - 设置缓存目录
   - 配置WSL2优化（如tmpfs）

### 9.3 训练执行阶段

1. **启动分布式训练**
   - 使用`torchrun`或`python -m torch.distributed.launch`
   - 配置正确的world_size和rank
   - 初始化进程组

2. **监控和调优**
   - 监控内存使用情况
   - 记录训练速度
   - 根据实际情况调整参数

### 9.4 性能评估

1. **基准测试**
   - 测试优化前的性能
   - 测试优化后的性能
   - 计算提升幅度

2. **内存分析**
   - 使用`memory_profiler`分析内存使用
   - 识别内存热点
   - 优化内存占用

## 十、预期效果总结

### 10.1 性能提升

| 指标 | 优化前 | 优化后 | 提升 |
|------|--------|--------|------|
| Phase 1训练速度 | 基准 | +30-50% | 1.3-1.5x |
| Phase 2环境吞吐 | 基准 | +20-30% | 1.2-1.3x |
| Phase 3训练速度 | 基准 | +40-60% | 1.4-1.6x |
| 内存占用峰值 | 15-16GB | 10-13GB | -20-35% |
| GPU利用率 | 60-70% | 80-95% | +20-35% |

### 10.2 稳定性提升

- **内存稳定性**：峰值内存控制在12-14GB以内，避免OOM
- **训练稳定性**：减少因内存不足导致的训练中断
- **数据一致性**：分布式训练时确保数据不重复、不遗漏

### 10.3 可扩展性提升

- **支持更大规模数据**：通过分块和内存映射，支持超过16GB的数据集
- **支持更多GPU**：分布式数据加载可扩展到4卡、8卡
- **支持更复杂模型**：节省的内存可用于更大的模型

## 十一、故障排查

### 11.1 常见问题

#### 问题1：内存不足（OOM）
**症状：** 训练过程中出现`RuntimeError: CUDA out of memory`

**解决方案：**
1. 减小batch_size
2. 减少num_workers
3. 启用混合精度训练
4. 增加gradient_accumulation
5. 检查是否有内存泄漏

#### 问题2：DataLoader速度慢
**症状：** GPU利用率低，等待数据

**解决方案：**
1. 增加num_workers（不超过CPU核心数/2）
2. 启用pin_memory
3. 增加prefetch_factor
4. 检查磁盘I/O性能
5. 使用更快的存储（SSD）

#### 问题3：WSL2 I/O慢
**症状：** 数据加载速度明显低于预期

**解决方案：**
1. 将数据移到WSL2文件系统
2. 使用tmpfs存储缓存
3. 避免跨文件系统访问
4. 使用HDF5或NPY格式

#### 问题4：分布式训练数据不一致
**症状：** 不同GPU训练结果差异大

**解决方案：**
1. 确保使用DistributedSampler
2. 设置固定随机种子
3. 检查数据分片是否正确
4. 验证每个进程的数据量

### 11.2 监控工具

#### 内存监控
```python
import psutil
import torch

def monitor_memory():
    # 系统内存
    mem = psutil.virtual_memory()
    print(f"系统内存: {mem.used/1024**3:.2f}GB / {mem.total/1024**3:.2f}GB")
    
    # GPU内存
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            print(f"GPU {i} 显存: {allocated:.2f}GB / {reserved:.2f}GB")
```

#### 性能分析
```python
import time
from contextlib import contextmanager

@contextmanager
def timer(name):
    start = time.time()
    yield
    elapsed = time.time() - start
    print(f"{name}: {elapsed:.4f}s")

# 使用示例
with timer("Data Loading"):
    batch = next(iter(dataloader))

with timer("Forward Pass"):
    output = model(batch)

with timer("Backward Pass"):
    loss.backward()
```

## 十二、总结

本优化策略针对以下核心约束进行了全面优化：

1. **内存约束（16GB）**：通过分块加载、内存映射、智能缓存，将峰值内存控制在12-14GB
2. **CPU资源（32线程）**：合理配置num_workers（4-6），平衡并发和内存
3. **GPU资源（2×22GB）**：优化batch_size和DataLoader，充分利用双卡
4. **WSL2环境**：优化文件I/O，避免跨文件系统访问，使用内存映射

通过以上优化，预期可实现：
- 训练速度提升30-60%
- 内存占用降低20-35%
- GPU利用率提升至80-95%
- 训练稳定性显著提高

所有策略均基于PyTorch最佳实践，结合项目特点进行了针对性优化，可直接应用于实际训练场景。
