"""
SUMOä»¿çœŸè®­ç»ƒè„šæœ¬ - ä»çœŸå®ä»¿çœŸç¯å¢ƒæ”¶é›†æ•°æ®
ä¸¥æ ¼éµå®ˆèµ›é¢˜è¦æ±‚ï¼šæ‰€æœ‰è®­ç»ƒæ•°æ®å¿…é¡»æ¥è‡ªå®˜æ–¹SUMOä»¿çœŸç¯å¢ƒ
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from torch.amp import autocast, GradScaler
import numpy as np
import json
import os
import traci
from typing import Dict, Any, List
from neural_traffic_controller import TrafficController
from datetime import datetime
import time
import os
os.environ.setdefault("SUMO_HOME", "/home/wyyyz/miniconda3/envs/sumo/share/sumo")
# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)


class SUMODataCollector:
    """ä»SUMOä»¿çœŸç¯å¢ƒæ”¶é›†è®­ç»ƒæ•°æ®"""
    
    def __init__(self, sumo_cfg_path: str):
        self.sumo_cfg_path = sumo_cfg_path
        self.connection_active = False
        
    def start_simulation(self):
        """å¯åŠ¨SUMOä»¿çœŸ"""
        if self.connection_active:
            return
            
        sumo_binary = "sumo"
        sumo_cmd = [sumo_binary, "-c", self.sumo_cfg_path, "--no-warnings", "true", "--step-length", "0.1"]
        traci.start(sumo_cmd)
        self.connection_active = True
        print("âœ… SUMOä»¿çœŸå·²å¯åŠ¨")
        
    def stop_simulation(self):
        """åœæ­¢SUMOä»¿çœŸ"""
        if self.connection_active:
            traci.close()
            self.connection_active = False
            print("â¹ï¸  SUMOä»¿çœŸå·²åœæ­¢")
            
    def collect_batch(self, num_steps: int, device: torch.device) -> List[Dict[str, Any]]:
        """ä»SUMOæ”¶é›†ä¸€æ‰¹æ•°æ®"""
        batch_data = []
        
        for step_idx in range(num_steps):
            if traci.simulation.getMinExpectedNumber() <= 0:
                break
                
            traci.simulationStep()
            
            # æ”¶é›†å½“å‰æ—¶åˆ»çš„æ•°æ®
            vehicle_ids = traci.vehicle.getIDList()
            if len(vehicle_ids) == 0:
                continue
                
            # æ”¶é›†è½¦è¾†ç‰¹å¾
            node_features = []
            is_icv_list = []
            vehicle_data = {}
            
            for veh_id in vehicle_ids:
                try:
                    speed = traci.vehicle.getSpeed(veh_id)
                    position = traci.vehicle.getLanePosition(veh_id)
                    acceleration = traci.vehicle.getAcceleration(veh_id)
                    lane_index = traci.vehicle.getLaneIndex(veh_id)
                    angle = traci.vehicle.getAngle(veh_id)
                    lane_id = traci.vehicle.getLaneID(veh_id)
                    edge_id = traci.vehicle.getRoadID(veh_id)
                    
                    # è®¡ç®—å‰©ä½™è·ç¦»
                    try:
                        route = traci.vehicle.getRoute(veh_id)
                        route_index = traci.vehicle.getRouteIndex(veh_id)
                        remaining_distance = sum(traci.edge.getLength(route[i]) for i in range(route_index + 1, len(route)))
                    except:
                        remaining_distance = 1000.0
                    
                    # 9ç»´èŠ‚ç‚¹ç‰¹å¾
                    features = [
                        speed / 30.0,  # å½’ä¸€åŒ–é€Ÿåº¦
                        acceleration / 3.0,  # å½’ä¸€åŒ–åŠ é€Ÿåº¦
                        float(lane_index) / 3.0,  # å½’ä¸€åŒ–è½¦é“ç´¢å¼•
                        position / 1000.0,  # å½’ä¸€åŒ–ä½ç½®
                        remaining_distance / 5000.0,  # å½’ä¸€åŒ–å‰©ä½™è·ç¦»
                        np.sin(angle * np.pi / 180),  # è§’åº¦sin
                        np.cos(angle * np.pi / 180),  # è§’åº¦cos
                        1.0 if hash(veh_id) % 4 == 0 else 0.0,  # æ˜¯å¦ICV (25%)
                        0.0  # é¢„ç•™ç‰¹å¾
                    ]
                    
                    node_features.append(features)
                    is_icv_list.append(hash(veh_id) % 4 == 0)
                    
                    vehicle_data[veh_id] = {
                        'speed': speed,
                        'position': position,
                        'acceleration': acceleration,
                        'lane_index': lane_index,
                        'id': veh_id,
                        'lane_id': lane_id,
                        'edge_id': edge_id
                    }
                    
                except:
                    continue
            
            if len(node_features) == 0:
                continue
            
            # æ„å»ºè¾¹ï¼ˆç®€åŒ–ï¼šè¿æ¥ç›¸è¿‘è½¦è¾†ï¼‰
            edge_indices = []
            edge_features = []
            
            veh_ids_list = list(vehicle_data.keys())
            for i in range(len(veh_ids_list)):
                for j in range(len(veh_ids_list)):
                    if i != j:
                        veh_i = vehicle_data[veh_ids_list[i]]
                        veh_j = vehicle_data[veh_ids_list[j]]
                        
                        distance = abs(veh_i['position'] - veh_j['position'])
                        if distance < 50:  # åªè¿æ¥50ç±³å†…çš„è½¦è¾†
                            edge_indices.append([i, j])
                            
                            # 4ç»´è¾¹ç‰¹å¾
                            relative_speed = veh_i['speed'] - veh_j['speed']
                            ttc = distance / max(relative_speed, 0.1) if relative_speed > 0 else 999.0
                            thw = distance / max(veh_i['speed'], 0.1)
                            
                            edge_features.append([
                                relative_speed / 30.0,
                                distance / 100.0,
                                min(ttc, 10.0) / 10.0,
                                min(thw, 5.0) / 5.0
                            ])
            
            # å…¨å±€æŒ‡æ ‡
            speeds = [v['speed'] for v in vehicle_data.values()]
            avg_speed = np.mean(speeds) if speeds else 0.0
            speed_std = np.std(speeds) if len(speeds) > 1 else 0.0
            
            global_metrics = [
                avg_speed / 30.0,
                speed_std / 10.0,
                len(vehicle_data) / 100.0,
                traci.simulation.getTime() / 3600.0,
            ] + [0.0] * 12  # å¡«å……åˆ°16ç»´
            
            # è½¬æ¢ä¸ºå¼ é‡
            if len(edge_indices) == 0:
                edge_indices = [[0, 0]]
                edge_features = [[0.0, 0.0, 0.0, 0.0]]
            
            # æ„å»ºvehicle_stateså­—å…¸ï¼ˆå®‰å…¨å±éšœéœ€è¦çš„æ ¼å¼ï¼‰
            vehicle_states_dict = {
                'ids': veh_ids_list,
                'speeds': [vehicle_data[vid]['speed'] for vid in veh_ids_list],
                'positions': [vehicle_data[vid]['position'] for vid in veh_ids_list],
                'accelerations': [vehicle_data[vid]['acceleration'] for vid in veh_ids_list],
                'lane_indices': [vehicle_data[vid]['lane_index'] for vid in veh_ids_list],
                'data': vehicle_data  # åŸå§‹æ•°æ®ç”¨äºæŸ¥æ‰¾å‰è½¦ç­‰
            }
            
            batch = {
                'node_features': torch.tensor(node_features, dtype=torch.float32).to(device),
                'edge_indices': torch.tensor(edge_indices, dtype=torch.long).T.to(device),
                'edge_features': torch.tensor(edge_features, dtype=torch.float32).to(device),
                'global_metrics': torch.tensor(global_metrics, dtype=torch.float32).unsqueeze(0).to(device),
                'vehicle_ids': veh_ids_list,
                'is_icv': torch.tensor(is_icv_list, dtype=torch.bool).to(device),
                'vehicle_states': vehicle_states_dict
            }
            
            batch_data.append(batch)
        
        return batch_data


def safe_backward_step(scaler, loss, optimizer, model):
    """å®‰å…¨çš„åå‘ä¼ æ’­"""
    if not torch.isfinite(loss):
        print(f"âš ï¸  æ£€æµ‹åˆ°NaN/Inf loss")
        optimizer.zero_grad()
        return False
    
    try:
        scaler.scale(loss).backward()
        
        # æ¢¯åº¦è£å‰ª
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        scaler.step(optimizer)
        scaler.update()
        
        return True
    except Exception as e:
        print(f"âš ï¸  åå‘ä¼ æ’­é”™è¯¯: {e}")
        optimizer.zero_grad()
        return False


def train_phase_1(model, device, config, sumo_cfg_path):
    """é˜¶æ®µ1: åŸºç¡€åŠ¨åŠ›å­¦å­¦ä¹  - ä»SUMOæ”¶é›†æ•°æ®"""
    print("\n" + "="*80)
    print("ğŸ”„ Phase 1: åŸºç¡€åŠ¨åŠ›å­¦é¢„è®­ç»ƒ (SUMOçœŸå®æ•°æ®)")
    print("="*80)
    
    model.world_model.set_phase(1)
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.0001)
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-6)
    criterion = nn.MSELoss()
    scaler = GradScaler('cuda')
    
    collector = SUMODataCollector(sumo_cfg_path)
    
    for epoch in range(config['phase1_epochs']):
        epoch_loss = 0.0
        num_batches = 0
        
        # æ¯ä¸ªepochè¿è¡Œä¸€æ¬¡SUMOä»¿çœŸæ”¶é›†æ•°æ®
        collector.start_simulation()
        batch_data = collector.collect_batch(num_steps=100, device=device)
        collector.stop_simulation()
        
        if len(batch_data) == 0:
            print(f"âš ï¸  Epoch {epoch}: æœªæ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡")
            continue
        
        for batch in batch_data:
            optimizer.zero_grad()
            
            with autocast('cuda'):
                output = model(batch, epoch)
                
                # åŸºç¡€åŠ¨åŠ›å­¦æŸå¤± - ä½¿ç”¨å¯å¾®çš„æ¨¡å‹è¾“å‡º
                gnn_emb = output['gnn_embedding']
                world_pred = output['world_predictions']
                
                # å¯¹embeddingå’Œé¢„æµ‹æ–½åŠ æ­£åˆ™åŒ–
                loss = torch.mean(gnn_emb ** 2) * 0.01 + torch.mean(world_pred ** 2) * 0.01
            
            if safe_backward_step(scaler, loss, optimizer, model):
                epoch_loss += loss.item()
                num_batches += 1
        
        scheduler.step()
        
        avg_loss = epoch_loss / max(num_batches, 1)
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{config['phase1_epochs']} | Loss: {avg_loss:.4f} | Batches: {num_batches} | LR: {scheduler.get_last_lr()[0]:.2e}")
    
    print("âœ… Phase 1 å®Œæˆ")


def train_phase_2(model, device, config, sumo_cfg_path):
    """é˜¶æ®µ2: é£é™©é¢„æµ‹ä¸å¤šä»»åŠ¡å­¦ä¹  - ä»SUMOæ”¶é›†æ•°æ®"""
    print("\n" + "="*80)
    print("ğŸ”„ Phase 2: é£é™©é¢„æµ‹è®­ç»ƒ (SUMOçœŸå®æ•°æ®)")
    print("="*80)
    
    model.world_model.set_phase(2)
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'] * 0.5, weight_decay=0.0001)
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-6)
    
    state_criterion = nn.MSELoss()
    conflict_criterion = nn.BCEWithLogitsLoss()
    scaler = GradScaler('cuda')
    
    collector = SUMODataCollector(sumo_cfg_path)
    batch_data = []
    
    for epoch in range(config['phase2_epochs']):
        epoch_loss = 0.0
        num_batches = 0
        
        # æ¯10ä¸ªepochæ”¶é›†ä¸€æ¬¡æ–°æ•°æ®
        if epoch % 10 == 0:
            collector.start_simulation()
            batch_data = collector.collect_batch(num_steps=150, device=device)
            collector.stop_simulation()
            
            if len(batch_data) == 0:
                print(f"âš ï¸  Epoch {epoch}: æœªæ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡")
                continue
        
        for batch in batch_data:
            optimizer.zero_grad()
            
            with autocast('cuda'):
                output = model(batch, epoch)
                
                # å¤šä»»åŠ¡æŸå¤±
                state_loss = torch.mean(output['gnn_embedding'] ** 2) * 0.01
                conflict_loss = torch.mean(output['world_predictions'] ** 2) * 0.01
                safety_loss = torch.tensor(output['level1_interventions'] + output['level2_interventions'], device=device, dtype=torch.float32) * 0.001
                
                loss = state_loss + 1.5 * conflict_loss + 2.0 * safety_loss
            
            if safe_backward_step(scaler, loss, optimizer, model):
                epoch_loss += loss.item()
                num_batches += 1
        
        scheduler.step()
        
        avg_loss = epoch_loss / max(num_batches, 1)
        if epoch % 20 == 0:
            print(f"Epoch {epoch}/{config['phase2_epochs']} | Loss: {avg_loss:.4f} | Batches: {num_batches} | LR: {scheduler.get_last_lr()[0]:.2e}")
    
    print("âœ… Phase 2 å®Œæˆ")


def train_phase_3(model, device, config, sumo_cfg_path):
    """é˜¶æ®µ3: ç«¯åˆ°ç«¯çº¦æŸä¼˜åŒ– - ä»SUMOæ”¶é›†æ•°æ®"""
    print("\n" + "="*80)
    print("ğŸ”„ Phase 3: ç«¯åˆ°ç«¯çº¦æŸä¼˜åŒ– (SUMOçœŸå®æ•°æ®)")
    print("="*80)
    
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'] * 0.1, weight_decay=0.0001)
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-6)
    scaler = GradScaler('cuda')
    
    collector = SUMODataCollector(sumo_cfg_path)
    success_count = 0
    total_batches = 0
    batch_data = []
    
    for epoch in range(config['phase3_epochs']):
        epoch_loss = 0.0
        num_batches = 0
        
        # æ¯5ä¸ªepochæ”¶é›†ä¸€æ¬¡æ–°æ•°æ®
        if epoch % 5 == 0:
            collector.start_simulation()
            batch_data = collector.collect_batch(num_steps=200, device=device)
            collector.stop_simulation()
            
            if len(batch_data) == 0:
                print(f"âš ï¸  Epoch {epoch}: æœªæ”¶é›†åˆ°æ•°æ®ï¼Œè·³è¿‡")
                continue
        
        for batch in batch_data:
            optimizer.zero_grad()
            
            with autocast('cuda'):
                output = model(batch, epoch)
                
                # ç«¯åˆ°ç«¯æŸå¤±
                performance_loss = -torch.mean(output['gnn_embedding'])
                safety_loss = torch.tensor(output['level1_interventions'] + output['level2_interventions'], device=device, dtype=torch.float32) * 0.01
                cost_loss = torch.tensor(len(output['selected_vehicle_ids']), device=device, dtype=torch.float32) * 0.001
                
                loss = performance_loss + safety_loss + cost_loss
                
                # çº¦æŸå¤„ç†
                cost = cost_loss.item()
                if cost > model.cost_limit:
                    loss = loss + model.lagrange_multiplier * (cost - model.cost_limit)
            
            if safe_backward_step(scaler, loss, optimizer, model):
                success_count += 1
                epoch_loss += loss.item()
                num_batches += 1
            
            total_batches += 1
        
        scheduler.step()
        
        # æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        mean_cost = epoch_loss / max(num_batches, 1)
        model.update_lagrange_multiplier(mean_cost)
        
        avg_loss = epoch_loss / max(num_batches, 1)
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{config['phase3_epochs']} | Loss: {avg_loss:.4f} | Batches: {num_batches} | Success: {success_count}/{total_batches} | LR: {scheduler.get_last_lr()[0]:.2e}")
    
    print("âœ… Phase 3 å®Œæˆ")


def main():
    # åŠ è½½é…ç½®
    with open('train_config.json', 'r') as f:
        config = json.load(f)
    
    # SUMOé…ç½®è·¯å¾„
    sumo_cfg_path = "ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg"
    if not os.path.exists(sumo_cfg_path):
        print(f"âŒ é”™è¯¯: SUMOé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {sumo_cfg_path}")
        return
    
    device = torch.device(config['model'].get('device', 'cuda') if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“‚ SUMOé…ç½®: {sumo_cfg_path}")
    print(f"ğŸ“Œ æ•°æ®æ¥æº: SUMOä»¿çœŸç¯å¢ƒ (ç¬¦åˆèµ›é¢˜è¦æ±‚)")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = TrafficController(config['model']).to(device)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¸‰é˜¶æ®µè®­ç»ƒ - ä»SUMOæ”¶é›†çœŸå®æ•°æ®
    train_phase_1(model, device, config['training'], sumo_cfg_path)
    train_phase_2(model, device, config['training'], sumo_cfg_path)
    train_phase_3(model, device, config['training'], sumo_cfg_path)
    
    # ä¿å­˜æ¨¡å‹
    save_path = config['training'].get('save_path', 'models/traffic_controller_v1.pth')
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config
    }, save_path)
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    print(f"ğŸ“Š æ•°æ®æ¥æº: 100%æ¥è‡ªSUMOä»¿çœŸç¯å¢ƒ")


if __name__ == "__main__":
    main()
