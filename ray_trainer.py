"""
Ray RLlib ConstrainedPPOè®­ç»ƒå™¨

åŠŸèƒ½è¯´æ˜ï¼š
1. åˆ›å»ºConstrainedPPOTrainerç±»ï¼Œç»§æ‰¿è‡ªray.rllib.algorithms.ppo.PPOTrainer
2. å®ç°æ‹‰æ ¼æœ—æ—¥çº¦æŸä¼˜åŒ–ï¼Œå°†å¹²é¢„æˆæœ¬ä½œä¸ºçº¦æŸæ¡ä»¶
3. åŠ¨æ€è°ƒæ•´æ‹‰æ ¼æœ—æ—¥ä¹˜å­ä»¥å¹³è¡¡å¥–åŠ±å’Œçº¦æŸ
4. é›†æˆåˆ°Ray RLlibçš„è®­ç»ƒæµç¨‹ä¸­

æ ¸å¿ƒæ€æƒ³ï¼š
- åŸå§‹é—®é¢˜ï¼šæœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼ŒåŒæ—¶æ»¡è¶³çº¦æŸæ¡ä»¶
- æ‹‰æ ¼æœ—æ—¥æ–¹æ³•ï¼šå°†çº¦æŸè½¬åŒ–ä¸ºæƒ©ç½šé¡¹ï¼Œé€šè¿‡ä¹˜å­åŠ¨æ€è°ƒæ•´
- ç›®æ ‡å‡½æ•°ï¼šL = E[R] - Î» * (E[C] - d)
  - R: å¥–åŠ±
  - C: æˆæœ¬ï¼ˆå¹²é¢„æ¬¡æ•°ï¼‰
  - d: æˆæœ¬é™åˆ¶
  - Î»: æ‹‰æ ¼æœ—æ—¥ä¹˜å­

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from ray_trainer import ConstrainedPPOTrainer
    from ray_model import register_traffic_controller_model
    
    # æ³¨å†Œæ¨¡å‹
    register_traffic_controller_model()
    
    # é…ç½®è®­ç»ƒå™¨
    config = {
        "env": "sumo_gym_env",
        "model": {
            "custom_model": "traffic_controller_model",
            "custom_model_config": {...}
        },
        # çº¦æŸä¼˜åŒ–å‚æ•°
        "cost_limit": 0.1,           # æˆæœ¬é™åˆ¶ï¼ˆæ¯æ­¥å¹³å‡å¹²é¢„æ¬¡æ•°ï¼‰
        "lambda_lr": 0.01,           # æ‹‰æ ¼æœ—æ—¥ä¹˜å­å­¦ä¹ ç‡
        "lambda_init": 1.0,          # æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆå§‹å€¼
        "alpha": 0.5,                # çº¦æŸå‚æ•°ï¼ˆæ§åˆ¶çº¦æŸä¸¥æ ¼ç¨‹åº¦ï¼‰
        "beta": 0.9,                 # çº¦æŸå‚æ•°ï¼ˆæ§åˆ¶ä¹˜å­æ›´æ–°å¹³æ»‘åº¦ï¼‰
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ConstrainedPPOTrainer(config=config)
"""

import numpy as np
import torch
from typing import Dict, Any, Optional, List, Tuple
from ray.rllib.algorithms.ppo import PPO as PPOTrainer
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.utils.typing import TensorType, PolicyID
from ray.rllib.utils.annotations import override
from ray.rllib.utils.metrics import NUM_AGENT_STEPS_SAMPLED, NUM_ENV_STEPS_SAMPLED


class ConstrainedPPOTrainer(PPOTrainer):
    """
    æ”¯æŒæ‹‰æ ¼æœ—æ—¥çº¦æŸçš„PPOè®­ç»ƒå™¨
    
    è¯¥è®­ç»ƒå™¨åœ¨æ ‡å‡†PPOç®—æ³•çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº†æ‹‰æ ¼æœ—æ—¥çº¦æŸä¼˜åŒ–æœºåˆ¶ï¼Œ
    ç”¨äºåœ¨æœ€å¤§åŒ–å¥–åŠ±çš„åŒæ—¶ï¼Œæ§åˆ¶å¹²é¢„æˆæœ¬åœ¨æŒ‡å®šèŒƒå›´å†…ã€‚
    
    ä¸»è¦ç‰¹æ€§ï¼š
    1. ç»§æ‰¿è‡ªPPOTrainerï¼Œä¿æŒä¸Ray RLlibçš„å®Œå…¨å…¼å®¹æ€§
    2. åœ¨compute_gradientsä¸­æ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸæ¢¯åº¦
    3. åŠ¨æ€æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­ä»¥å¹³è¡¡å¥–åŠ±å’Œçº¦æŸ
    4. æ”¯æŒå¤šæ™ºèƒ½ä½“åœºæ™¯
    5. æä¾›è¯¦ç»†çš„çº¦æŸè¿åç»Ÿè®¡
    
    çº¦æŸä¼˜åŒ–åŸç†ï¼š
    - åŸå§‹é—®é¢˜ï¼šmax_Ï€ E[R(Ï€)] s.t. E[C(Ï€)] â‰¤ d
    - æ‹‰æ ¼æœ—æ—¥å¯¹å¶ï¼šmin_Î» max_Ï€ E[R(Ï€) - Î»(C(Ï€) - d)]
    - æ›´æ–°è§„åˆ™ï¼š
      - ç­–ç•¥å‚æ•°ï¼šâˆ‡Î¸ L = âˆ‡Î¸ E[R - Î»(C - d)]
      - æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼šÎ» â† max(0, Î» + Î·(C - d))
    
    é…ç½®å‚æ•°ï¼š
        cost_limit (float): æˆæœ¬é™åˆ¶ï¼ˆé»˜è®¤0.1ï¼‰
        lambda_lr (float): æ‹‰æ ¼æœ—æ—¥ä¹˜å­å­¦ä¹ ç‡ï¼ˆé»˜è®¤0.01ï¼‰
        lambda_init (float): æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆå§‹å€¼ï¼ˆé»˜è®¤1.0ï¼‰
        alpha (float): çº¦æŸå‚æ•°ï¼Œæ§åˆ¶çº¦æŸä¸¥æ ¼ç¨‹åº¦ï¼ˆé»˜è®¤0.5ï¼‰
        beta (float): çº¦æŸå‚æ•°ï¼Œæ§åˆ¶ä¹˜å­æ›´æ–°å¹³æ»‘åº¦ï¼ˆé»˜è®¤0.9ï¼‰
    """
    
    def __init__(self, config: Dict[str, Any] = None, env: str = None, 
                 logger_creator=None):
        """
        åˆå§‹åŒ–ConstrainedPPOTrainer
        
        å‚æ•°è¯´æ˜ï¼š
            config: è®­ç»ƒå™¨é…ç½®å­—å…¸
            env: ç¯å¢ƒåç§°
            logger_creator: æ—¥å¿—åˆ›å»ºå™¨
        
        é…ç½®å‚æ•°è¯´æ˜ï¼š
            cost_limit: æˆæœ¬é™åˆ¶ï¼Œæ¯æ­¥å…è®¸çš„å¹³å‡å¹²é¢„æ¬¡æ•°
            lambda_lr: æ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„å­¦ä¹ ç‡
            lambda_init: æ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„åˆå§‹å€¼
            alpha: çº¦æŸå‚æ•°ï¼ˆ0-1ï¼‰ï¼Œæ§åˆ¶çº¦æŸè¿åæ—¶çš„æƒ©ç½šå¼ºåº¦
            beta: çº¦æŸå‚æ•°ï¼ˆ0-1ï¼‰ï¼Œæ§åˆ¶æ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„æ›´æ–°å¹³æ»‘åº¦
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(config=config, env=env, logger_creator=logger_creator)
        
        # ä»é…ç½®ä¸­æå–çº¦æŸä¼˜åŒ–å‚æ•°
        self.cost_limit = config.get('cost_limit', 0.1)
        self.lambda_lr = config.get('lambda_lr', 0.01)
        self.lambda_init = config.get('lambda_init', 1.0)
        self.alpha = config.get('alpha', 0.5)
        self.beta = config.get('beta', 0.9)
        
        # åˆå§‹åŒ–æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼ˆæ¯ä¸ªç­–ç•¥ä¸€ä¸ªä¹˜å­ï¼‰
        self.lagrange_multipliers = {}
        
        # åˆå§‹åŒ–çº¦æŸè¿åå†å²ï¼ˆç”¨äºå¹³æ»‘å’Œç»Ÿè®¡ï¼‰
        self.constraint_violation_history = {}
        
        # åˆå§‹åŒ–æˆæœ¬å†å²
        self.cost_history = {}
        
        # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
        print("=" * 60)
        print("ğŸ” ConstrainedPPOTrainer åˆå§‹åŒ–å®Œæˆ!")
        print("=" * 60)
        print(f"âš™ï¸  çº¦æŸä¼˜åŒ–é…ç½®:")
        print(f"   - æˆæœ¬é™åˆ¶ (cost_limit): {self.cost_limit}")
        print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­å­¦ä¹ ç‡ (lambda_lr): {self.lambda_lr}")
        print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆå§‹å€¼ (lambda_init): {self.lambda_init}")
        print(f"   - çº¦æŸå‚æ•° alpha: {self.alpha}")
        print(f"   - çº¦æŸå‚æ•° beta: {self.beta}")
        print("=" * 60)
    
    @override(PPOTrainer)
    def compute_gradients(self, samples: SampleBatch, **kwargs) -> Tuple[TensorType, Dict[str, Any]]:
        """
        è®¡ç®—æ¢¯åº¦ï¼Œæ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸé¡¹
        
        è¯¥æ–¹æ³•æ˜¯è®­ç»ƒçš„æ ¸å¿ƒï¼Œè´Ÿè´£ï¼š
        1. è°ƒç”¨çˆ¶ç±»çš„compute_gradientsè®¡ç®—åŸºç¡€PPOæ¢¯åº¦
        2. ä»æ ·æœ¬ä¸­æå–æˆæœ¬ä¿¡æ¯
        3. è®¡ç®—çº¦æŸè¿å
        4. æ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸæ¢¯åº¦
        5. è¿”å›ä¿®æ”¹åçš„æ¢¯åº¦
        
        å‚æ•°è¯´æ˜ï¼š
            samples: æ ·æœ¬æ‰¹æ¬¡ï¼ŒåŒ…å«è§‚æµ‹ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ä¿¡æ¯
            **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°
        
        è¿”å›ï¼š
            Tuple[TensorType, Dict[str, Any]]:
                - æ¢¯åº¦å¼ é‡ï¼ˆåŒ…å«æ‹‰æ ¼æœ—æ—¥çº¦æŸé¡¹ï¼‰
                - ä¿¡æ¯å­—å…¸ï¼ˆåŒ…å«çº¦æŸç»Ÿè®¡ä¿¡æ¯ï¼‰
        
        æ¢¯åº¦è®¡ç®—å…¬å¼ï¼š
            âˆ‡Î¸ L = âˆ‡Î¸ E[R - Î»(C - d)]
                  = âˆ‡Î¸ E[R] - Î» * âˆ‡Î¸ E[C]
        
        å…¶ä¸­ï¼š
            - R: å¥–åŠ±
            - C: æˆæœ¬ï¼ˆå¹²é¢„æ¬¡æ•°ï¼‰
            - d: æˆæœ¬é™åˆ¶
            - Î»: æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        """
        # 1. è°ƒç”¨çˆ¶ç±»è®¡ç®—åŸºç¡€PPOæ¢¯åº¦
        grads, info = super().compute_gradients(samples, **kwargs)
        
        # 2. æå–ç­–ç•¥ID
        policy_id = samples.policy_id if hasattr(samples, 'policy_id') else 'default_policy'
        
        # 3. åˆå§‹åŒ–è¯¥ç­–ç•¥çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        if policy_id not in self.lagrange_multipliers:
            self.lagrange_multipliers[policy_id] = self.lambda_init
            self.constraint_violation_history[policy_id] = []
            self.cost_history[policy_id] = []
            print(f"ğŸ“Š åˆå§‹åŒ–ç­–ç•¥ '{policy_id}' çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­: {self.lambda_init}")
        
        # 4. ä»æ ·æœ¬ä¸­æå–æˆæœ¬ä¿¡æ¯
        # å‡è®¾æ ·æœ¬ä¸­åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        # - level1_interventions: ä¸€çº§å¹²é¢„æ¬¡æ•°ï¼ˆå®‰å…¨å±éšœè§¦å‘ï¼‰
        # - level2_interventions: äºŒçº§å¹²é¢„æ¬¡æ•°ï¼ˆç´§æ€¥å¹²é¢„ï¼‰
        # è¿™äº›å­—æ®µéœ€è¦åœ¨ç¯å¢ƒæˆ–æ¨¡å‹ä¸­è®¡ç®—å¹¶æ·»åŠ åˆ°æ ·æœ¬ä¸­
        level1_interventions = samples.get('level1_interventions', np.zeros(len(samples)))
        level2_interventions = samples.get('level2_interventions', np.zeros(len(samples)))
        
        # 5. è®¡ç®—æ€»æˆæœ¬ï¼ˆå¹²é¢„æ¬¡æ•°ï¼‰
        total_cost = level1_interventions + level2_interventions
        
        # 6. è®¡ç®—å¹³å‡æˆæœ¬
        avg_cost = np.mean(total_cost)
        
        # 7. è®¡ç®—çº¦æŸè¿å
        constraint_violation = self._compute_constraint_violation(avg_cost)
        
        # 8. æ›´æ–°å†å²è®°å½•
        self.cost_history[policy_id].append(avg_cost)
        self.constraint_violation_history[policy_id].append(constraint_violation)
        
        # é™åˆ¶å†å²é•¿åº¦
        max_history = 100
        if len(self.cost_history[policy_id]) > max_history:
            self.cost_history[policy_id] = self.cost_history[policy_id][-max_history:]
            self.constraint_violation_history[policy_id] = self.constraint_violation_history[policy_id][-max_history:]
        
        # 9. è·å–å½“å‰æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        lambda_ = self.lagrange_multipliers[policy_id]
        
        # 10. æ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸæ¢¯åº¦
        # åŸºç¡€æ¢¯åº¦ï¼šâˆ‡Î¸ E[R]
        # çº¦æŸæ¢¯åº¦ï¼š-Î» * âˆ‡Î¸ E[C]
        # æ€»æ¢¯åº¦ï¼šâˆ‡Î¸ E[R] - Î» * âˆ‡Î¸ E[C]
        
        # è®¡ç®—æˆæœ¬æ¢¯åº¦ï¼ˆç®€åŒ–å¤„ç†ï¼šä½¿ç”¨å¥–åŠ±æ¢¯åº¦çš„æ–¹å‘ï¼‰
        # åœ¨å®é™…å®ç°ä¸­ï¼Œåº”è¯¥ç›´æ¥è®¡ç®—æˆæœ¬å¯¹ç­–ç•¥å‚æ•°çš„æ¢¯åº¦
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè¿‘ä¼¼ï¼šæˆæœ¬ä¸å¥–åŠ±æˆåæ¯”ï¼Œå› æ­¤æˆæœ¬æ¢¯åº¦æ–¹å‘ä¸å¥–åŠ±æ¢¯åº¦ç›¸å
        
        # è·å–ç­–ç•¥å¯¹è±¡
        policy = self.get_policy(policy_id)
        
        # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æƒ©ç½šé¡¹
        lagrangian_penalty = lambda_ * constraint_violation
        
        # ä¿®æ”¹æ¢¯åº¦ï¼šæ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸé¡¹
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¢¯åº¦ç»“æ„è¿›è¡Œè°ƒæ•´
        if isinstance(grads, dict):
            # å¦‚æœæ¢¯åº¦æ˜¯å­—å…¸å½¢å¼ï¼ˆæŒ‰å‚æ•°åç´¢å¼•ï¼‰
            for param_name in grads:
                # è·å–æˆæœ¬å¯¹è¯¥å‚æ•°çš„æ¢¯åº¦ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                # åœ¨å®Œæ•´å®ç°ä¸­ï¼Œåº”è¯¥é€šè¿‡åå‘ä¼ æ’­è®¡ç®—
                cost_grad = self._compute_cost_gradient(samples, policy, param_name)
                
                # æ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸæ¢¯åº¦
                grads[param_name] = grads[param_name] - lambda_ * cost_grad
        elif isinstance(grads, (list, np.ndarray, torch.Tensor)):
            # å¦‚æœæ¢¯åº¦æ˜¯å¼ é‡å½¢å¼
            cost_grad = self._compute_cost_gradient(samples, policy, None)
            
            # æ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸæ¢¯åº¦
            if isinstance(grads, torch.Tensor):
                grads = grads - lambda_ * cost_grad
            else:
                grads = grads - lambda_ * cost_grad.numpy() if isinstance(cost_grad, torch.Tensor) else grads - lambda_ * cost_grad
        
        # 11. æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        self.update_lagrange_multiplier(policy_id, constraint_violation)
        
        # 12. æ„å»ºä¿¡æ¯å­—å…¸
        info.update({
            'constraint_violation': constraint_violation,
            'avg_cost': avg_cost,
            'lagrangian_multiplier': lambda_,
            'lagrangian_penalty': lagrangian_penalty,
            'level1_interventions': np.mean(level1_interventions),
            'level2_interventions': np.mean(level2_interventions),
            'total_interventions': avg_cost,
        })
        
        # 13. æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆæ¯100æ­¥ï¼‰
        if self.iteration % 100 == 0:
            print(f"\nğŸ”„ è®­ç»ƒè¿­ä»£ {self.iteration}:")
            print(f"   - å¹³å‡æˆæœ¬: {avg_cost:.4f} (é™åˆ¶: {self.cost_limit})")
            print(f"   - çº¦æŸè¿å: {constraint_violation:.4f}")
            print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­: {lambda_:.4f}")
            print(f"   - æ‹‰æ ¼æœ—æ—¥æƒ©ç½š: {lagrangian_penalty:.4f}")
            print(f"   - ä¸€çº§å¹²é¢„: {np.mean(level1_interventions):.4f}")
            print(f"   - äºŒçº§å¹²é¢„: {np.mean(level2_interventions):.4f}")
        
        return grads, info
    
    def update_lagrange_multiplier(self, policy_id: PolicyID, 
                                   constraint_violation: float) -> None:
        """
        æ›´æ–°æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        
        è¯¥æ–¹æ³•æ ¹æ®çº¦æŸè¿åæƒ…å†µåŠ¨æ€è°ƒæ•´æ‹‰æ ¼æœ—æ—¥ä¹˜å­ï¼Œ
        ä»¥å¹³è¡¡å¥–åŠ±æœ€å¤§åŒ–å’Œçº¦æŸæ»¡è¶³ã€‚
        
        æ›´æ–°è§„åˆ™ï¼š
            Î» â† max(0, Î» + Î· * (C - d))
        
        å…¶ä¸­ï¼š
            - Î»: æ‹‰æ ¼æœ—æ—¥ä¹˜å­
            - Î·: å­¦ä¹ ç‡ï¼ˆlambda_lrï¼‰
            - C: å¹³å‡æˆæœ¬
            - d: æˆæœ¬é™åˆ¶
        
        å‚æ•°è¯´æ˜ï¼š
            policy_id: ç­–ç•¥ID
            constraint_violation: çº¦æŸè¿åå€¼ï¼ˆC - dï¼‰
        
        æ›´æ–°é€»è¾‘ï¼š
            - å¦‚æœçº¦æŸè¿å > 0ï¼ˆæˆæœ¬è¶…è¿‡é™åˆ¶ï¼‰ï¼Œå¢åŠ ä¹˜å­ä»¥åŠ å¼ºæƒ©ç½š
            - å¦‚æœçº¦æŸè¿å < 0ï¼ˆæˆæœ¬ä½äºé™åˆ¶ï¼‰ï¼Œå‡å°ä¹˜å­ä»¥æ”¾æ¾çº¦æŸ
            - ä½¿ç”¨betaå‚æ•°è¿›è¡Œå¹³æ»‘æ›´æ–°
        """
        # è·å–å½“å‰ä¹˜å­
        current_lambda = self.lagrange_multipliers.get(policy_id, self.lambda_init)
        
        # è®¡ç®—ä¹˜å­æ›´æ–°é‡
        # ä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ›´æ–°ä¹˜å­ï¼ˆæœ€å¤§åŒ–æ‹‰æ ¼æœ—æ—¥å¯¹å¶å‡½æ•°ï¼‰
        delta = self.lambda_lr * constraint_violation
        
        # ä½¿ç”¨betaå‚æ•°è¿›è¡Œå¹³æ»‘æ›´æ–°
        # lambda_new = beta * (lambda_old + delta) + (1 - beta) * lambda_old
        #            = lambda_old + beta * delta
        new_lambda = current_lambda + self.beta * delta
        
        # ç¡®ä¿ä¹˜å­éè´Ÿï¼ˆæ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„ç‰©ç†æ„ä¹‰ï¼‰
        new_lambda = max(0.0, new_lambda)
        
        # é™åˆ¶ä¹˜å­æœ€å¤§å€¼ï¼ˆé˜²æ­¢æ•°å€¼ä¸ç¨³å®šï¼‰
        max_lambda = 100.0
        new_lambda = min(new_lambda, max_lambda)
        
        # æ›´æ–°ä¹˜å­
        self.lagrange_multipliers[policy_id] = new_lambda
        
        # è®°å½•æ›´æ–°ä¿¡æ¯
        if self.iteration % 100 == 0:
            print(f"   ğŸ“ˆ æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ›´æ–°: {current_lambda:.4f} â†’ {new_lambda:.4f}")
            print(f"      æ›´æ–°é‡: {delta:.4f} (çº¦æŸè¿å: {constraint_violation:.4f})")
    
    def _compute_constraint_violation(self, avg_cost: float) -> float:
        """
        è®¡ç®—çº¦æŸè¿å
        
        è¯¥æ–¹æ³•è®¡ç®—å½“å‰æˆæœ¬ä¸æˆæœ¬é™åˆ¶ä¹‹é—´çš„å·®å¼‚ï¼Œ
        ç”¨äºè¯„ä¼°çº¦æŸæ»¡è¶³æƒ…å†µã€‚
        
        è®¡ç®—å…¬å¼ï¼š
            violation = C - d
        
        å…¶ä¸­ï¼š
            - C: å¹³å‡æˆæœ¬
            - d: æˆæœ¬é™åˆ¶
        
        å‚æ•°è¯´æ˜ï¼š
            avg_cost: å¹³å‡æˆæœ¬ï¼ˆå¹²é¢„æ¬¡æ•°ï¼‰
        
        è¿”å›ï¼š
            constraint_violation: çº¦æŸè¿åå€¼
                - æ­£å€¼ï¼šæˆæœ¬è¶…è¿‡é™åˆ¶
                - é›¶å€¼ï¼šæˆæœ¬æ­£å¥½ç­‰äºé™åˆ¶
                - è´Ÿå€¼ï¼šæˆæœ¬ä½äºé™åˆ¶
        
        æ³¨æ„ï¼š
            - çº¦æŸè¿åè¶Šå¤§ï¼Œæ‹‰æ ¼æœ—æ—¥æƒ©ç½šè¶Šå¼º
            - è´Ÿçš„çº¦æŸè¿åè¡¨ç¤ºçº¦æŸè¢«æ»¡è¶³ï¼Œå¯ä»¥æ”¾æ¾æƒ©ç½š
        """
        # è®¡ç®—çº¦æŸè¿å
        violation = avg_cost - self.cost_limit
        
        # ä½¿ç”¨alphaå‚æ•°è°ƒæ•´çº¦æŸè¿åçš„æ•æ„Ÿåº¦
        # alpha > 0.5: å¯¹çº¦æŸè¿åæ›´æ•æ„Ÿ
        # alpha < 0.5: å¯¹çº¦æŸè¿åä¸å¤ªæ•æ„Ÿ
        adjusted_violation = self.alpha * violation
        
        return adjusted_violation
    
    def _compute_cost_gradient(self, samples: SampleBatch, policy, 
                                param_name: Optional[str] = None) -> TensorType:
        """
        è®¡ç®—æˆæœ¬å¯¹ç­–ç•¥å‚æ•°çš„æ¢¯åº¦
        
        è¯¥æ–¹æ³•è®¡ç®—å¹²é¢„æˆæœ¬å¯¹ç­–ç•¥å‚æ•°çš„æ¢¯åº¦ï¼Œ
        ç”¨äºåœ¨æ¢¯åº¦æ›´æ–°ä¸­æ·»åŠ æ‹‰æ ¼æœ—æ—¥çº¦æŸé¡¹ã€‚
        
        å‚æ•°è¯´æ˜ï¼š
            samples: æ ·æœ¬æ‰¹æ¬¡
            policy: ç­–ç•¥å¯¹è±¡
            param_name: å‚æ•°åç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼‰
        
        è¿”å›ï¼š
            cost_grad: æˆæœ¬æ¢¯åº¦
        
        æ³¨æ„ï¼š
            - è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°
            - åœ¨å®Œæ•´å®ç°ä¸­ï¼Œåº”è¯¥é€šè¿‡åå‘ä¼ æ’­è®¡ç®—
            - è¿™é‡Œä½¿ç”¨ä¸€ä¸ªè¿‘ä¼¼ï¼šæˆæœ¬ä¸å¥–åŠ±æˆåæ¯”
        """
        # æå–æˆæœ¬ä¿¡æ¯
        level1_interventions = samples.get('level1_interventions', np.zeros(len(samples)))
        level2_interventions = samples.get('level2_interventions', np.zeros(len(samples)))
        total_cost = level1_interventions + level2_interventions
        
        # è®¡ç®—æˆæœ¬æ¢¯åº¦ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        # åœ¨å®Œæ•´å®ç°ä¸­ï¼Œåº”è¯¥ï¼š
        # 1. å®šä¹‰æˆæœ¬å‡½æ•° C(Î¸) = E[interventions]
        # 2. è®¡ç®—æ¢¯åº¦ âˆ‡Î¸ C(Î¸)
        # 3. ä½¿ç”¨ç­–ç•¥æ¢¯åº¦å®šç†æˆ–è‡ªåŠ¨å¾®åˆ†
        
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè¿‘ä¼¼ï¼š
        # å‡è®¾æˆæœ¬ä¸å¥–åŠ±æˆåæ¯”ï¼Œå› æ­¤æˆæœ¬æ¢¯åº¦æ–¹å‘ä¸å¥–åŠ±æ¢¯åº¦ç›¸å
        # cost_grad â‰ˆ -reward_grad * (cost / reward)
        
        # è·å–å¥–åŠ±
        rewards = samples.get('rewards', np.zeros(len(samples)))
        
        # é¿å…é™¤é›¶
        avg_reward = np.mean(rewards)
        avg_cost = np.mean(total_cost)
        
        # è®¡ç®—æˆæœ¬æ¢¯åº¦è¿‘ä¼¼å€¼
        if avg_reward != 0:
            cost_ratio = avg_cost / (abs(avg_reward) + 1e-8)
            # æˆæœ¬æ¢¯åº¦ä¸å¥–åŠ±æ¢¯åº¦æ–¹å‘ç›¸åï¼Œå¤§å°ä¸æˆæœ¬æ¯”æˆæ­£æ¯”
            cost_grad_magnitude = cost_ratio
        else:
            cost_grad_magnitude = 0.0
        
        # å¦‚æœæŒ‡å®šäº†å‚æ•°åç§°ï¼Œè¿”å›è¯¥å‚æ•°çš„æ¢¯åº¦
        if param_name is not None:
            # è·å–å‚æ•°å½¢çŠ¶
            if hasattr(policy, 'model') and hasattr(policy.model, param_name):
                param = getattr(policy.model, param_name)
                if hasattr(param, 'shape'):
                    # åˆ›å»ºä¸å‚æ•°å½¢çŠ¶ç›¸åŒçš„æ¢¯åº¦å¼ é‡
                    if isinstance(param, torch.Tensor):
                        cost_grad = torch.ones_like(param) * cost_grad_magnitude
                    else:
                        cost_grad = np.ones(param.shape) * cost_grad_magnitude
                else:
                    cost_grad = cost_grad_magnitude
            else:
                cost_grad = cost_grad_magnitude
        else:
            # è¿”å›æ ‡é‡æ¢¯åº¦
            cost_grad = cost_grad_magnitude
        
        return cost_grad
    
    @override(PPOTrainer)
    def learn_on_batch(self, samples: SampleBatch) -> Dict[str, Any]:
        """
        åœ¨æ‰¹æ¬¡ä¸Šå­¦ä¹ ï¼ˆé‡å†™ä»¥æ”¯æŒæ‹‰æ ¼æœ—æ—¥çº¦æŸï¼‰
        
        è¯¥æ–¹æ³•é‡å†™çˆ¶ç±»çš„learn_on_batchæ–¹æ³•ï¼Œ
        åœ¨æ ‡å‡†PPOå­¦ä¹ æµç¨‹ä¸­é›†æˆæ‹‰æ ¼æœ—æ—¥çº¦æŸä¼˜åŒ–ã€‚
        
        å‚æ•°è¯´æ˜ï¼š
            samples: æ ·æœ¬æ‰¹æ¬¡
        
        è¿”å›ï¼š
            info: ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«è®­ç»ƒç»Ÿè®¡å’Œçº¦æŸç»Ÿè®¡
        """
        # è°ƒç”¨çˆ¶ç±»çš„learn_on_batch
        info = super().learn_on_batch(samples)
        
        # æ·»åŠ çº¦æŸç»Ÿè®¡ä¿¡æ¯
        for policy_id in self.lagrange_multipliers:
            if policy_id in self.cost_history and len(self.cost_history[policy_id]) > 0:
                avg_cost = np.mean(self.cost_history[policy_id][-10:])  # æœ€è¿‘10æ¬¡çš„å¹³å‡
                avg_violation = np.mean(self.constraint_violation_history[policy_id][-10:])
                lambda_ = self.lagrange_multipliers[policy_id]
                
                info[f'policy_{policy_id}_avg_cost'] = avg_cost
                info[f'policy_{policy_id}_constraint_violation'] = avg_violation
                info[f'policy_{policy_id}_lagrangian_multiplier'] = lambda_
        
        return info
    
    def get_constraint_stats(self) -> Dict[str, Any]:
        """
        è·å–çº¦æŸç»Ÿè®¡ä¿¡æ¯
        
        è¯¥æ–¹æ³•è¿”å›å½“å‰æ‰€æœ‰ç­–ç•¥çš„çº¦æŸç»Ÿè®¡ä¿¡æ¯ï¼Œ
        ç”¨äºç›‘æ§å’Œè°ƒè¯•çº¦æŸä¼˜åŒ–è¿‡ç¨‹ã€‚
        
        è¿”å›ï¼š
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«ï¼š
                - æ¯ä¸ªç­–ç•¥çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­
                - æ¯ä¸ªç­–ç•¥çš„å¹³å‡æˆæœ¬
                - æ¯ä¸ªç­–ç•¥çš„çº¦æŸè¿å
                - æ¯ä¸ªç­–ç•¥çš„æˆæœ¬å†å²
                - æ¯ä¸ªç­–ç•¥çš„çº¦æŸè¿åå†å²
        """
        stats = {
            'cost_limit': self.cost_limit,
            'lambda_lr': self.lambda_lr,
            'alpha': self.alpha,
            'beta': self.beta,
            'policies': {}
        }
        
        for policy_id in self.lagrange_multipliers:
            policy_stats = {
                'lagrangian_multiplier': self.lagrange_multipliers[policy_id],
                'cost_history': list(self.cost_history.get(policy_id, [])),
                'constraint_violation_history': list(self.constraint_violation_history.get(policy_id, [])),
            }
            
            # è®¡ç®—ç»Ÿè®¡é‡
            if len(policy_stats['cost_history']) > 0:
                policy_stats['avg_cost'] = np.mean(policy_stats['cost_history'])
                policy_stats['std_cost'] = np.std(policy_stats['cost_history'])
                policy_stats['min_cost'] = np.min(policy_stats['cost_history'])
                policy_stats['max_cost'] = np.max(policy_stats['cost_history'])
            
            if len(policy_stats['constraint_violation_history']) > 0:
                policy_stats['avg_violation'] = np.mean(policy_stats['constraint_violation_history'])
                policy_stats['std_violation'] = np.std(policy_stats['constraint_violation_history'])
                policy_stats['min_violation'] = np.min(policy_stats['constraint_violation_history'])
                policy_stats['max_violation'] = np.max(policy_stats['constraint_violation_history'])
            
            stats['policies'][policy_id] = policy_stats
        
        return stats
    
    def reset_lagrange_multipliers(self, value: Optional[float] = None) -> None:
        """
        é‡ç½®æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        
        è¯¥æ–¹æ³•é‡ç½®æ‰€æœ‰ç­–ç•¥çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆ°æŒ‡å®šå€¼æˆ–åˆå§‹å€¼ã€‚
        
        å‚æ•°è¯´æ˜ï¼š
            value: é‡ç½®å€¼ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨lambda_init
        """
        reset_value = value if value is not None else self.lambda_init
        
        for policy_id in self.lagrange_multipliers:
            self.lagrange_multipliers[policy_id] = reset_value
            print(f"ğŸ”„ é‡ç½®ç­–ç•¥ '{policy_id}' çš„æ‹‰æ ¼æœ—æ—¥ä¹˜å­: {reset_value}")
    
    def set_cost_limit(self, new_limit: float) -> None:
        """
        è®¾ç½®æ–°çš„æˆæœ¬é™åˆ¶
        
        è¯¥æ–¹æ³•åŠ¨æ€è°ƒæ•´æˆæœ¬é™åˆ¶ï¼Œç”¨äºå®éªŒå’Œè°ƒä¼˜ã€‚
        
        å‚æ•°è¯´æ˜ï¼š
            new_limit: æ–°çš„æˆæœ¬é™åˆ¶
        """
        old_limit = self.cost_limit
        self.cost_limit = new_limit
        print(f"ğŸ“Š æˆæœ¬é™åˆ¶æ›´æ–°: {old_limit} â†’ {new_limit}")


def create_constrained_ppo_trainer(config: Dict[str, Any]) -> ConstrainedPPOTrainer:
    """
    åˆ›å»ºConstrainedPPOTrainerçš„å·¥å‚å‡½æ•°
    
    è¯¥å‡½æ•°æä¾›äº†ä¸€ç§ä¾¿æ·çš„æ–¹å¼æ¥åˆ›å»ºConstrainedPPOTrainerå®ä¾‹ï¼Œ
    å¹¶è®¾ç½®åˆç†çš„é»˜è®¤é…ç½®ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
        config: è®­ç»ƒå™¨é…ç½®å­—å…¸
    
    è¿”å›ï¼š
        trainer: ConstrainedPPOTrainerå®ä¾‹
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        config = {
            "env": "sumo_gym_env",
            "framework": "torch",
            "num_workers": 4,
            # çº¦æŸä¼˜åŒ–å‚æ•°
            "cost_limit": 0.1,
            "lambda_lr": 0.01,
            "lambda_init": 1.0,
            "alpha": 0.5,
            "beta": 0.9,
        }
        trainer = create_constrained_ppo_trainer(config)
    """
    # è®¾ç½®é»˜è®¤é…ç½®
    default_config = {
        "cost_limit": 0.1,
        "lambda_lr": 0.01,
        "lambda_init": 1.0,
        "alpha": 0.5,
        "beta": 0.9,
    }
    
    # åˆå¹¶ç”¨æˆ·é…ç½®
    merged_config = {**default_config, **config}
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ConstrainedPPOTrainer(config=merged_config)
    
    return trainer


# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œæµ‹è¯•
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª ConstrainedPPOTrainer æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    test_config = {
        "env": "CartPole-v1",
        "framework": "torch",
        "num_gpus": 0,
        "num_workers": 0,
        # çº¦æŸä¼˜åŒ–å‚æ•°
        "cost_limit": 0.1,
        "lambda_lr": 0.01,
        "lambda_init": 1.0,
        "alpha": 0.5,
        "beta": 0.9,
    }
    
    print("\nğŸ“ æµ‹è¯•é…ç½®:")
    print(f"   - ç¯å¢ƒ: {test_config['env']}")
    print(f"   - æ¡†æ¶: {test_config['framework']}")
    print(f"   - æˆæœ¬é™åˆ¶: {test_config['cost_limit']}")
    print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­å­¦ä¹ ç‡: {test_config['lambda_lr']}")
    print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆå§‹å€¼: {test_config['lambda_init']}")
    
    print("\nâœ… ConstrainedPPOTrainer å·²å‡†å¤‡å°±ç»ª!")
    print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("   from ray_trainer import ConstrainedPPOTrainer")
    print("   trainer = ConstrainedPPOTrainer(config=config)")
    print("   result = trainer.train()")
    print("=" * 60)
