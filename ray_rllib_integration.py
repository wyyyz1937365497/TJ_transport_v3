"""
Ray RLlib é›†æˆæ¨¡å—
åˆ©ç”¨ Ray RLlib çš„å¤šè¿›ç¨‹å¹¶è¡Œèƒ½åŠ›ï¼ŒåŒæ—¶è·‘å¤šä¸ª SUMO å®ä¾‹
å®ç°åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import numpy as np
import gymnasium as gym
from typing import Dict, List, Tuple, Any, Optional
import os
import time

try:
    import ray
    from ray import tune
    from ray.rllib.algorithms.ppo import PPO
    from ray.rllib.models import ModelCatalog
    from ray.rllib.models.tf.tf_modelv2 import TFModelV2
    from ray.rllib.utils import try_import_tf
    from ray.tune.registry import register_env
    tf = try_import_tf()
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False
    print("âš ï¸  Ray RLlib æœªå®‰è£…ï¼Œåˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…å‘½ä»¤: pip install ray[rllib]")


from sumo_rl_env import SUMORLEnvironment
from neural_traffic_controller import TrafficController


class SUMORayEnvironment(gym.Env):
    """
    Ray RLlib å…¼å®¹çš„ SUMO ç¯å¢ƒ
    å®ç° Gymnasium æ ‡å‡†æ¥å£
    """
    
    metadata = {
        'render_modes': ['human', 'rgb_array'],
        'render_fps': 10
    }
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ– Ray SUMO ç¯å¢ƒ
        
        Args:
            config: ç¯å¢ƒé…ç½®å­—å…¸
        """
        if config is None:
            config = {}
        
        self.config = config
        
        # SUMO é…ç½®
        self.sumo_cfg_path = config.get('sumo_cfg_path', 'ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg')
        self.use_gui = config.get('use_gui', False)
        self.max_steps = config.get('max_steps', 3600)
        self.seed_val = config.get('seed', None)
        
        # åˆå§‹åŒ– SUMO ç¯å¢ƒ
        self.sumo_env = SUMORLEnvironment(
            sumo_cfg_path=self.sumo_cfg_path,
            use_gui=self.use_gui,
            max_steps=self.max_steps,
            seed=self.seed_val
        )
        
        # åŠ¨ä½œç©ºé—´
        # åŠ¨ä½œæ˜¯å­—å…¸å½¢å¼ï¼Œä½†åœ¨ RLlib ä¸­éœ€è¦å±•å¹³
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ MultiDiscrete æˆ–è€… Box ç©ºé—´
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨è¿ç»­åŠ¨ä½œç©ºé—´ [åŠ é€Ÿåº¦, æ¢é“æ¦‚ç‡] * top_k
        self.top_k = config.get('top_k', 5)
        self.action_dim = 2 * self.top_k  # [accel1, lane1, accel2, lane2, ...]
        
        self.action_space = gym.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.action_dim,),
            dtype=np.float32
        )
        
        # è§‚æµ‹ç©ºé—´ - åŠ¨æ€è®¡ç®—
        # åŒ…å«ï¼šèŠ‚ç‚¹ç‰¹å¾ã€è¾¹ç‰¹å¾ã€å…¨å±€æŒ‡æ ‡
        self.observation_space = gym.spaces.Dict({
            'node_features': gym.spaces.Box(
                low=-np.inf,
                high=np.inf,
                shape=(None, 9),  # åŠ¨æ€è½¦è¾†æ•°
                dtype=np.float32
            ),
            'edge_indices': gym.spaces.Box(
                low=0,
                high=np.iinfo(np.int32).max,
                shape=(2, None),  # åŠ¨æ€è¾¹æ•°
                dtype=np.int32
            ),
            'edge_features': gym.spaces.Box(
                low=-np.inf,
                high=np.inf,
                shape=(None, 4),  # åŠ¨æ€è¾¹æ•°
                dtype=np.float32
            ),
            'global_metrics': gym.spaces.Box(
                low=-np.inf,
                high=np.inf,
                shape=(16,),
                dtype=np.float32
            ),
            'is_icv': gym.spaces.Box(
                low=0,
                high=1,
                shape=(None,),  # åŠ¨æ€è½¦è¾†æ•°
                dtype=np.int8
            )
        })
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_step = 0
        self.episode_reward = 0.0
        self.vehicle_ids = []
        
        print(f"âœ… Ray SUMO ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦: {self.action_dim}")
        print(f"   Top-K: {self.top_k}")
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[Dict, Dict]:
        """
        é‡ç½®ç¯å¢ƒ
        
        Args:
            seed: éšæœºç§å­
            options: é¢å¤–é€‰é¡¹
            
        Returns:
            observation: åˆå§‹è§‚æµ‹
            info: é¢å¤–ä¿¡æ¯
        """
        if seed is not None:
            self.seed_val = seed
        
        # é‡ç½® SUMO ç¯å¢ƒ
        observation = self.sumo_env.reset()
        
        # è½¬æ¢ä¸º Gym æ ¼å¼
        gym_observation = self._convert_to_gym_observation(observation)
        
        # é‡ç½®çŠ¶æ€
        self.current_step = 0
        self.episode_reward = 0.0
        self.vehicle_ids = observation['vehicle_ids']
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'vehicle_count': len(self.vehicle_ids),
            'step': self.current_step
        }
        
        return gym_observation, info
    
    def step(self, action: np.ndarray) -> Tuple[Dict, float, bool, bool, Dict]:
        """
        æ‰§è¡Œä¸€æ­¥
        
        Args:
            action: åŠ¨ä½œæ•°ç»„ [accel1, lane1, accel2, lane2, ...]
            
        Returns:
            observation: è§‚æµ‹
            reward: å¥–åŠ±
            terminated: æ˜¯å¦ç»ˆæ­¢ï¼ˆæ­£å¸¸ç»“æŸï¼‰
            truncated: æ˜¯å¦æˆªæ–­ï¼ˆè¶…æ—¶ç­‰ï¼‰
            info: é¢å¤–ä¿¡æ¯
        """
        # é‡å¡‘åŠ¨ä½œ
        action = action.reshape(-1, 2)  # [K, 2]
        
        # æ„å»º SUMO ç¯å¢ƒæœŸæœ›çš„åŠ¨ä½œæ ¼å¼
        # é€‰æ‹©å‰ top_k è¾†è½¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        selected_vehicle_ids = self.vehicle_ids[:self.top_k] if len(self.vehicle_ids) >= self.top_k else self.vehicle_ids
        
        safe_actions = torch.tensor(action[:len(selected_vehicle_ids)], dtype=torch.float32)
        
        sumo_action = {
            'selected_vehicle_ids': selected_vehicle_ids,
            'safe_actions': safe_actions
        }
        
        # æ‰§è¡Œä¸€æ­¥
        observation, reward, done, info = self.sumo_env.step(sumo_action)
        
        # è½¬æ¢è§‚æµ‹
        gym_observation = self._convert_to_gym_observation(observation)
        
        # æ›´æ–°çŠ¶æ€
        self.current_step += 1
        self.episode_reward += reward
        self.vehicle_ids = observation['vehicle_ids']
        
        # åˆ¤æ–­ç»ˆæ­¢å’Œæˆªæ–­
        terminated = done
        truncated = self.current_step >= self.max_steps
        
        # æ›´æ–°é¢å¤–ä¿¡æ¯
        info.update({
            'episode_reward': self.episode_reward,
            'step': self.current_step,
            'vehicle_count': len(self.vehicle_ids)
        })
        
        return gym_observation, reward, terminated, truncated, info
    
    def _convert_to_gym_observation(self, observation: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """
        å°† SUMO è§‚æµ‹è½¬æ¢ä¸º Gym æ ¼å¼
        
        Args:
            observation: SUMO è§‚æµ‹
            
        Returns:
            gym_observation: Gym æ ¼å¼è§‚æµ‹
        """
        vehicle_data = observation['vehicle_data']
        vehicle_ids = observation['vehicle_ids']
        
        if not vehicle_ids:
            return {
                'node_features': np.zeros((0, 9), dtype=np.float32),
                'edge_indices': np.zeros((2, 0), dtype=np.int32),
                'edge_features': np.zeros((0, 4), dtype=np.float32),
                'global_metrics': np.array(observation['global_metrics'], dtype=np.float32),
                'is_icv': np.zeros((0,), dtype=np.int8)
            }
        
        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾
        node_features = []
        is_icv_list = []
        
        for veh_id in vehicle_ids:
            vehicle = vehicle_data[veh_id]
            features = [
                vehicle.get('position', 0.0),
                vehicle.get('speed', 0.0),
                vehicle.get('acceleration', 0.0),
                vehicle.get('lane_index', 0),
                1000.0,  # å‰©ä½™è·ç¦»ï¼ˆç®€åŒ–ï¼‰
                0.5,  # å®Œæˆç‡ï¼ˆç®€åŒ–ï¼‰
                1.0 if vehicle.get('is_icv', False) else 0.0,
                self.current_step * 0.1,
                0.1
            ]
            node_features.append(features)
            is_icv_list.append(1 if vehicle.get('is_icv', False) else 0)
        
        # æ„å»ºè¾¹ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        edge_indices = []
        edge_features = []
        
        for i, veh_id_i in enumerate(vehicle_ids):
            for j, veh_id_j in enumerate(vehicle_ids):
                if i == j:
                    continue
                
                pos_i = vehicle_data[veh_id_i].get('position', 0.0)
                pos_j = vehicle_data[veh_id_j].get('position', 0.0)
                speed_i = vehicle_data[veh_id_i].get('speed', 0.0)
                speed_j = vehicle_data[veh_id_j].get('speed', 0.0)
                
                distance = abs(pos_i - pos_j)
                if distance < 50:
                    edge_indices.append([i, j])
                    
                    rel_distance = distance
                    rel_speed = abs(speed_i - speed_j)
                    
                    ttc = rel_distance / max(rel_speed, 0.1) if rel_speed > 0 else 100
                    thw = rel_distance / max(speed_i, 0.1) if speed_i > 0 else 100
                    
                    edge_features.append([rel_distance, rel_speed, min(ttc, 10), min(thw, 10)])
        
        # è½¬æ¢ä¸º numpy æ•°ç»„
        gym_observation = {
            'node_features': np.array(node_features, dtype=np.float32),
            'edge_indices': np.array(edge_indices, dtype=np.int32).T if edge_indices else np.zeros((2, 0), dtype=np.int32),
            'edge_features': np.array(edge_features, dtype=np.float32) if edge_features else np.zeros((0, 4), dtype=np.float32),
            'global_metrics': np.array(observation['global_metrics'], dtype=np.float32),
            'is_icv': np.array(is_icv_list, dtype=np.int8)
        }
        
        return gym_observation
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.sumo_env.close()
    
    def render(self, mode: str = 'human'):
        """
        æ¸²æŸ“ç¯å¢ƒ
        
        Args:
            mode: æ¸²æŸ“æ¨¡å¼
        """
        if mode == 'human' and self.use_gui:
            # SUMO GUI å·²ç»åœ¨è¿è¡Œ
            pass
        elif mode == 'rgb_array':
            # è¿”å›æˆªå›¾ï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰
            pass
    
    def get_episode_statistics(self) -> Dict[str, float]:
        """è·å– episode ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_steps': self.current_step,
            'total_reward': self.episode_reward,
            'avg_reward': self.episode_reward / max(self.current_step, 1),
            'vehicle_count': len(self.vehicle_ids)
        }


def create_ray_sumo_env(config: Dict[str, Any]) -> SUMORayEnvironment:
    """
    åˆ›å»º Ray SUMO ç¯å¢ƒçš„å·¥å‚å‡½æ•°
    
    Args:
        config: ç¯å¢ƒé…ç½®
        
    Returns:
        Ray SUMO ç¯å¢ƒå®ä¾‹
    """
    return SUMORayEnvironment(config)


class RayRLlibTrainer:
    """
    Ray RLlib è®­ç»ƒå™¨
    å®ç°åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– Ray RLlib è®­ç»ƒå™¨
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        if not RAY_AVAILABLE:
            raise ImportError("Ray RLlib æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
        
        self.config = config
        
        # åˆå§‹åŒ– Ray
        if not ray.is_initialized():
            ray.init(
                num_cpus=config.get('num_cpus', 8),
                num_gpus=config.get('num_gpus', 1),
                ignore_reinit_error=True
            )
            print(f"âœ… Ray åˆå§‹åŒ–å®Œæˆ")
            print(f"   CPUs: {config.get('num_cpus', 8)}")
            print(f"   GPUs: {config.get('num_gpus', 1)}")
        
        # æ³¨å†Œç¯å¢ƒ
        register_env("sumo_ray", create_ray_sumo_env)
        
        # é…ç½®ç®—æ³•
        self.algorithm = PPO(config=self._get_rllib_config())
        
        print("âœ… Ray RLlib è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _get_rllib_config(self) -> Dict[str, Any]:
        """
        è·å– RLlib é…ç½®
        
        Returns:
            RLlib é…ç½®å­—å…¸
        """
        config = {
            # ç¯å¢ƒé…ç½®
            "env": "sumo_ray",
            "env_config": {
                "sumo_cfg_path": self.config.get('sumo_cfg_path', 'ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg'),
                "use_gui": self.config.get('use_gui', False),
                "max_steps": self.config.get('max_steps', 3600),
                "seed": self.config.get('seed', None),
                "top_k": self.config.get('top_k', 5)
            },
            
            # å¹¶è¡Œé…ç½®
            "num_workers": self.config.get('num_workers', 4),  # å¹¶è¡Œç¯å¢ƒæ•°
            "num_envs_per_worker": self.config.get('num_envs_per_worker', 2),  # æ¯ä¸ªworkerçš„ç¯å¢ƒæ•°
            "train_batch_size": self.config.get('train_batch_size', 4000),
            "sgd_minibatch_size": self.config.get('sgd_minibatch_size', 128),
            
            # PPO é…ç½®
            "lr": self.config.get('learning_rate', 3e-4),
            "gamma": self.config.get('gamma', 0.99),
            "lambda": self.config.get('lambda', 0.95),
            "clip_param": self.config.get('clip_param', 0.2),
            "vf_clip_param": self.config.get('vf_clip_param', 10.0),
            "entropy_coeff": self.config.get('entropy_coeff', 0.01),
            "vf_loss_coeff": self.config.get('vf_loss_coeff', 0.5),
            
            # ç½‘ç»œé…ç½®
            "model": {
                "fcnet_hiddens": [256, 256, 128],
                "fcnet_activation": "relu",
                "vf_share_layers": True,
            },
            
            # è®­ç»ƒé…ç½®
            "num_sgd_iter": self.config.get('num_sgd_iter', 10),
            "framework": "torch",
            
            # èµ„æºé…ç½®
            "num_gpus": self.config.get('num_gpus', 1),
            "num_cpus_per_worker": self.config.get('num_cpus_per_worker', 1),
        }
        
        return config
    
    def train(self, num_iterations: int = 100):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            num_iterations: è®­ç»ƒè¿­ä»£æ¬¡æ•°
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹ Ray RLlib åˆ†å¸ƒå¼è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"è¿­ä»£æ¬¡æ•°: {num_iterations}")
        print(f"å¹¶è¡Œç¯å¢ƒæ•°: {self.config.get('num_workers', 4) * self.config.get('num_envs_per_worker', 2)}")
        print(f"{'='*60}\n")
        
        for i in range(num_iterations):
            # æ‰§è¡Œä¸€æ¬¡è®­ç»ƒè¿­ä»£
            result = self.algorithm.train()
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"\nè¿­ä»£ {i+1}/{num_iterations}:")
            print(f"  å¹³å‡å¥–åŠ±: {result['episode_reward_mean']:.4f}")
            print(f"  æœ€å°å¥–åŠ±: {result['episode_reward_min']:.4f}")
            print(f"  æœ€å¤§å¥–åŠ±: {result['episode_reward_max']:.4f}")
            print(f"  Episode é•¿åº¦: {result['episode_len_mean']:.2f}")
            print(f"  å­¦ä¹ ç‡: {result['info']['learner']['cur_lr']:.6f}")
            print(f"  ç†µ: {result['info']['learner']['entropy']:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (i + 1) % 10 == 0:
                checkpoint_path = self.algorithm.save()
                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"{'='*60}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_checkpoint = self.algorithm.save()
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_checkpoint}")
        
        return final_checkpoint
    
    def evaluate(self, num_episodes: int = 10):
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            num_episodes: è¯„ä¼° episode æ•°
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¼€å§‹è¯„ä¼°")
        print(f"{'='*60}")
        print(f"Episode æ•°: {num_episodes}")
        print(f"{'='*60}\n")
        
        total_rewards = []
        total_steps = []
        
        for episode in range(num_episodes):
            # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
            env = SUMORayEnvironment(self.config)
            obs, info = env.reset()
            
            episode_reward = 0.0
            done = False
            truncated = False
            steps = 0
            
            while not (done or truncated):
                # è·å–åŠ¨ä½œ
                action = self.algorithm.compute_single_action(obs)
                
                # æ‰§è¡Œä¸€æ­¥
                obs, reward, done, truncated, info = env.step(action)
                
                episode_reward += reward
                steps += 1
            
            total_rewards.append(episode_reward)
            total_steps.append(steps)
            
            print(f"Episode {episode+1}/{num_episodes}: "
                  f"å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={steps}")
            
            env.close()
        
        # ç»Ÿè®¡ç»“æœ
        avg_reward = np.mean(total_rewards)
        std_reward = np.std(total_rewards)
        avg_steps = np.mean(total_steps)
        
        print(f"\n{'='*60}")
        print("ğŸ“Š è¯„ä¼°ç»“æœ")
        print(f"{'='*60}")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.4f} Â± {std_reward:.4f}")
        print(f"å¹³å‡æ­¥æ•°: {avg_steps:.2f}")
        print(f"{'='*60}")
        
        return {
            'avg_reward': avg_reward,
            'std_reward': std_reward,
            'avg_steps': avg_steps,
            'all_rewards': total_rewards
        }
    
    def close(self):
        """å…³é—­è®­ç»ƒå™¨"""
        self.algorithm.stop()
        if ray.is_initialized():
            ray.shutdown()
        print("âœ… Ray RLlib è®­ç»ƒå™¨å·²å…³é—­")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤º Ray RLlib é›†æˆ"""
    if not RAY_AVAILABLE:
        print("âŒ Ray RLlib æœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        print("   å®‰è£…å‘½ä»¤: pip install ray[rllib]")
        return
    
    # è®­ç»ƒé…ç½®
    config = {
        'sumo_cfg_path': 'ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg',
        'use_gui': False,
        'max_steps': 3600,
        'seed': 42,
        'top_k': 5,
        
        # Ray é…ç½®
        'num_cpus': 8,
        'num_gpus': 1,
        'num_workers': 4,
        'num_envs_per_worker': 2,
        
        # è®­ç»ƒé…ç½®
        'learning_rate': 3e-4,
        'gamma': 0.99,
        'lambda': 0.95,
        'clip_param': 0.2,
        'train_batch_size': 4000,
        'sgd_minibatch_size': 128,
        'num_sgd_iter': 10,
        
        # è¯„ä¼°é…ç½®
        'num_evaluation_episodes': 10
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RayRLlibTrainer(config)
    
    # è®­ç»ƒ
    try:
        checkpoint_path = trainer.train(num_iterations=100)
        
        # è¯„ä¼°
        eval_results = trainer.evaluate(num_episodes=10)
        
        print(f"\nâœ… è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!")
        print(f"   æœ€ç»ˆæ¨¡å‹: {checkpoint_path}")
        print(f"   è¯„ä¼°å¥–åŠ±: {eval_results['avg_reward']:.4f}")
        
    finally:
        trainer.close()


if __name__ == "__main__":
    main()
