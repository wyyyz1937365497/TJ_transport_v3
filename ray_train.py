"""
Ray RLlib è®­ç»ƒè„šæœ¬ - SUMOäº¤é€šæ§åˆ¶

åŠŸèƒ½è¯´æ˜ï¼š
1. é›†æˆSUMO-RLç¯å¢ƒï¼ˆsumo_gym_env.pyï¼‰å’Œè‡ªå®šä¹‰æ¨¡å‹åŒ…è£…å™¨ï¼ˆray_model.pyï¼‰
2. é›†æˆConstrainedPPOè®­ç»ƒå™¨ï¼ˆray_trainer.pyï¼‰
3. é…ç½®Ray RolloutWorkersï¼ˆ4ä¸ªå¹¶è¡ŒSUMOè¿›ç¨‹ï¼‰
4. é…ç½®å¼‚æ­¥è®­ç»ƒæ¶æ„ï¼Œå®ç°æ—¶é—´é‡å 
5. å¯ç”¨LIBSUMO_AS_TRACIåŠ é€Ÿå’Œæ‰¹é‡è®¢é˜…
6. é…ç½®GPUè®­ç»ƒè¿›ç¨‹ï¼Œå®ç°å¼‚æ­¥æ¨¡å‹æ›´æ–°
7. å®ç°å®æ—¶æ•°æ®æ”¶é›†ï¼Œä¸æ”¯æŒä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
8. åŒ…å«æ•°æ®éªŒè¯ã€æ¢¯åº¦è£å‰ªã€æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
9. å®ç°åŸºäºæ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„å¥–åŠ±é‡å¡‘é€»è¾‘
10. é…ç½®æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼šnum_workersã€train_batch_sizeã€rollout_fragment_length
11. æ·»åŠ è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—å’Œè¿›åº¦æ˜¾ç¤º
12. å®ç°æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤
13. æ·»åŠ TensorBoardæ—¥å¿—è®°å½•

è®­ç»ƒæµç¨‹ï¼š
- Ray Driverå¯åŠ¨å¤šä¸ªRolloutWorkers
- æ¯ä¸ªWorkerè¿è¡Œç‹¬ç«‹çš„SUMOå®ä¾‹
- Workerså¹¶è¡Œæ”¶é›†rolloutæ•°æ®
- GPUè®­ç»ƒè¿›ç¨‹å¼‚æ­¥æ›´æ–°æ¨¡å‹
- å®ç°æ—¶é—´é‡å ï¼šSUMOç”Ÿæˆæ–°æ•°æ®çš„åŒæ—¶ï¼ŒGPUä½¿ç”¨æ—§æ•°æ®è®­ç»ƒ

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python ray_train.py --config config.json --restore checkpoint_path
"""

import os
import sys
import time
import json
import argparse
import logging
import numpy as np
from typing import Dict, Any, Optional, List
from datetime import datetime

# Ray RLlibå¯¼å…¥
import ray
from ray import tune
from ray.tune import CLIReporter
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.models import ModelCatalog
from ray.rllib.utils.metrics import NUM_AGENT_STEPS_SAMPLED, NUM_ENV_STEPS_SAMPLED

# æœ¬åœ°å¯¼å…¥
from sumo_gym_env import SUMOGymEnv, create_sumo_gym_env
from ray_model import (
    TrafficControllerModel,
    TrafficControllerModelV2,
    register_traffic_controller_model
)
from ray_trainer import ConstrainedPPOTrainer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# ç¯å¢ƒé…ç½®
# ============================================================================

def get_default_config() -> Dict[str, Any]:
    """
    è·å–é»˜è®¤è®­ç»ƒé…ç½®
    
    Returns:
        config: é»˜è®¤é…ç½®å­—å…¸
    """
    return {
        # ==================== åŸºç¡€é…ç½® ====================
        "framework": "torch",
        "env": SUMOGymEnv,
        
        # ==================== SUMOç¯å¢ƒé…ç½® ====================
        "sumo_cfg_path": "ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg",
        "use_libsumo": True,  # å¯ç”¨LIBSUMO_AS_TRACIåŠ é€Ÿ
        "batch_subscribe": True,  # å¯ç”¨æ‰¹é‡è®¢é˜…
        "max_steps": 3600,  # æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°
        "use_gui": False,
        
        # ==================== Rayå¹¶è¡Œé…ç½® ====================
        "num_workers": 4,  # 4ä¸ªå¹¶è¡ŒRolloutWorkers
        "num_gpus": 1,  # ä½¿ç”¨1ä¸ªGPUè¿›è¡Œè®­ç»ƒ
        "num_cpus_per_worker": 1,
        "num_envs_per_worker": 1,  # æ¯ä¸ªWorkerè¿è¡Œ1ä¸ªSUMOå®ä¾‹
        "worker_use_gpu": False,  # Workersä½¿ç”¨CPUï¼Œè®­ç»ƒè¿›ç¨‹ä½¿ç”¨GPU
        
        # ==================== å¼‚æ­¥è®­ç»ƒé…ç½® ====================
        "train_batch_size": 4000,  # æ¯æ¬¡è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ•°
        "rollout_fragment_length": 200,  # æ¯ä¸ªrolloutç‰‡æ®µçš„é•¿åº¦
        "sgd_minibatch_size": 128,  # SGDå°æ‰¹æ¬¡å¤§å°
        "num_sgd_iter": 10,  # æ¯æ¬¡è®­ç»ƒè¿­ä»£çš„SGDæ›´æ–°æ¬¡æ•°
        
        # ==================== PPOç®—æ³•é…ç½® ====================
        "lr": 3e-4,  # å­¦ä¹ ç‡
        "gamma": 0.99,  # æŠ˜æ‰£å› å­
        "lambda_": 0.95,  # GAEå‚æ•°
        "clip_param": 0.2,  # PPOè£å‰ªå‚æ•°
        "vf_loss_coeff": 0.5,  # ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°
        "entropy_coeff": 0.01,  # ç†µæ­£åˆ™åŒ–ç³»æ•°
        "kl_coeff": 0.2,  # KLæ•£åº¦ç³»æ•°
        "kl_target": 0.01,  # KLæ•£åº¦ç›®æ ‡å€¼
        
        # ==================== æ¢¯åº¦ä¼˜åŒ–é…ç½® ====================
        "grad_clip": 0.5,  # æ¢¯åº¦è£å‰ªé˜ˆå€¼
        "use_amp": True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
        
        # ==================== çº¦æŸä¼˜åŒ–é…ç½® ====================
        "cost_limit": 0.1,  # æˆæœ¬é™åˆ¶ï¼ˆæ¯æ­¥å¹³å‡å¹²é¢„æ¬¡æ•°ï¼‰
        "lambda_lr": 0.01,  # æ‹‰æ ¼æœ—æ—¥ä¹˜å­å­¦ä¹ ç‡
        "lambda_init": 1.0,  # æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆå§‹å€¼
        "alpha": 0.5,  # çº¦æŸå‚æ•°
        "beta": 0.9,  # çº¦æŸå‚æ•°
        
        # ==================== æ¨¡å‹é…ç½® ====================
        "model": {
            "custom_model": "traffic_controller_model",
            "custom_model_config": {
                "node_dim": 9,
                "edge_dim": 4,
                "gnn_hidden_dim": 64,
                "gnn_output_dim": 256,
                "gnn_layers": 3,
                "gnn_heads": 4,
                "world_hidden_dim": 128,
                "future_steps": 5,
                "controller_hidden_dim": 128,
                "global_dim": 16,
                "top_k": 5,
                "action_dim": 2,
                # å®‰å…¨å‚æ•°
                "ttc_threshold": 2.0,
                "thw_threshold": 1.5,
                "max_accel": 2.0,
                "max_decel": -3.0,
                "emergency_decel": -5.0,
                "max_lane_change_speed": 5.0,
                # çº¦æŸä¼˜åŒ–å‚æ•°
                "cost_limit": 0.1,
                "lambda_lr": 0.01,
            }
        },
        
        # ==================== æ£€æŸ¥ç‚¹é…ç½® ====================
        "checkpoint_freq": 10,  # æ¯10æ¬¡è¿­ä»£ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        "checkpoint_at_end": True,  # è®­ç»ƒç»“æŸæ—¶ä¿å­˜æ£€æŸ¥ç‚¹
        "keep_checkpoints_num": 5,  # ä¿ç•™æœ€è¿‘5ä¸ªæ£€æŸ¥ç‚¹
        "checkpoint_score_attr": "episode_reward_mean",
        
        # ==================== æ—¥å¿—é…ç½® ====================
        "log_level": "INFO",
        "log_dir": "./ray_results",
        "experiment_name": "sumo_traffic_control",
        
        # ==================== TensorBoardé…ç½® ====================
        "tensorboard_log": True,
        
        # ==================== è®­ç»ƒé…ç½® ====================
        "stop": {
            "training_iteration": 1000,  # æœ€å¤§è®­ç»ƒè¿­ä»£æ¬¡æ•°
            "episode_reward_mean": 100.0,  # è¾¾åˆ°æ­¤å¹³å‡å¥–åŠ±æ—¶åœæ­¢
        },
        
        # ==================== æ¢å¤é…ç½® ====================
        "restore": None,  # æ£€æŸ¥ç‚¹è·¯å¾„
    }


def load_config_from_file(config_path: str) -> Dict[str, Any]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½é…ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        config: é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    logger.info(f"âœ… ä» {config_path} åŠ è½½é…ç½®")
    return config


def merge_configs(default_config: Dict[str, Any], 
                  user_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
    
    Args:
        default_config: é»˜è®¤é…ç½®
        user_config: ç”¨æˆ·é…ç½®
        
    Returns:
        merged_config: åˆå¹¶åçš„é…ç½®
    """
    merged = default_config.copy()
    
    # é€’å½’åˆå¹¶åµŒå¥—å­—å…¸
    for key, value in user_config.items():
        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
            merged[key] = merge_configs(merged[key], value)
        else:
            merged[key] = value
    
    return merged


# ============================================================================
# ç¯å¢ƒåˆ›å»ºå‡½æ•°
# ============================================================================

def env_creator(env_config: Dict[str, Any]) -> SUMOGymEnv:
    """
    ç¯å¢ƒåˆ›å»ºå‡½æ•°ï¼ˆRay RLlibæ¥å£ï¼‰
    
    Args:
        env_config: ç¯å¢ƒé…ç½®å­—å…¸
        
    Returns:
        env: SUMOGymEnvå®ä¾‹
    """
    # ä»env_configä¸­æå–SUMOé…ç½®
    sumo_cfg_path = env_config.get("sumo_cfg_path", "ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg")
    use_libsumo = env_config.get("use_libsumo", True)
    batch_subscribe = env_config.get("batch_subscribe", True)
    max_steps = env_config.get("max_steps", 3600)
    use_gui = env_config.get("use_gui", False)
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = env_config.get("model_config", {})
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_sumo_gym_env(
        sumo_cfg_path=sumo_cfg_path,
        use_libsumo=use_libsumo,
        batch_subscribe=batch_subscribe,
        device='cpu',  # Workersä½¿ç”¨CPU
        model_config=model_config,
        max_steps=max_steps,
        use_gui=use_gui
    )
    
    logger.info(f"âœ… åˆ›å»ºSUMOç¯å¢ƒ: {sumo_cfg_path}")
    return env


# ============================================================================
# è‡ªå®šä¹‰å›è°ƒå‡½æ•°
# ============================================================================

class TrainingCallback:
    """
    è®­ç»ƒå›è°ƒå‡½æ•°
    
    ç”¨äºç›‘æ§è®­ç»ƒè¿›åº¦ã€è®°å½•æ—¥å¿—ã€ä¿å­˜æ£€æŸ¥ç‚¹ç­‰
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å›è°ƒå‡½æ•°
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self.start_time = time.time()
        self.best_reward = -np.inf
        self.best_iteration = 0
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_dir = config.get("log_dir", "./ray_results")
        self.experiment_name = config.get("experiment_name", "sumo_traffic_control")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = os.path.join(self.log_dir, f"{self.experiment_name}_{timestamp}")
        os.makedirs(self.run_dir, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        log_file = os.path.join(self.run_dir, "training.log")
        self.file_handler = logging.FileHandler(log_file)
        self.file_handler.setLevel(logging.INFO)
        self.file_handler.setFormatter(
            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        )
        logger.addHandler(self.file_handler)
        
        logger.info(f"âœ… è®­ç»ƒå›è°ƒåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è¿è¡Œç›®å½•: {self.run_dir}")
    
    def on_train_result(self, result: Dict[str, Any]) -> None:
        """
        è®­ç»ƒç»“æœå›è°ƒ
        
        Args:
            result: è®­ç»ƒç»“æœå­—å…¸
        """
        iteration = result.get("training_iteration", 0)
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        elapsed_time = time.time() - self.start_time
        
        # æå–å…³é”®æŒ‡æ ‡
        episode_reward_mean = result.get("episode_reward_mean", 0.0)
        episode_len_mean = result.get("episode_len_mean", 0.0)
        
        # æå–çº¦æŸç»Ÿè®¡
        constraint_violation = result.get("constraint_violation", 0.0)
        avg_cost = result.get("avg_cost", 0.0)
        lagrangian_multiplier = result.get("lagrangian_multiplier", 0.0)
        
        # æå–å®‰å…¨æŒ‡æ ‡
        ttc_violations = result.get("ttc_violations", 0)
        thw_violations = result.get("thw_violations", 0)
        
        # æå–è®­ç»ƒç»Ÿè®¡
        agent_steps = result.get(NUM_AGENT_STEPS_SAMPLED, 0)
        env_steps = result.get(NUM_ENV_STEPS_SAMPLED, 0)
        
        # æ‰“å°è®­ç»ƒè¿›åº¦
        print("\n" + "=" * 80)
        print(f"ğŸš€ è®­ç»ƒè¿­ä»£ {iteration}")
        print("=" * 80)
        print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {elapsed_time:.2f}ç§’")
        print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"   - å¹³å‡å¥–åŠ±: {episode_reward_mean:.4f}")
        print(f"   - å¹³å‡Episodeé•¿åº¦: {episode_len_mean:.2f}")
        print(f"ğŸ›¡ï¸  å®‰å…¨æŒ‡æ ‡:")
        print(f"   - TTCè¿è§„: {ttc_violations}")
        print(f"   - THWè¿è§„: {thw_violations}")
        print(f"ğŸ” çº¦æŸä¼˜åŒ–:")
        print(f"   - çº¦æŸè¿å: {constraint_violation:.4f}")
        print(f"   - å¹³å‡æˆæœ¬: {avg_cost:.4f} (é™åˆ¶: {self.config['cost_limit']})")
        print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­: {lagrangian_multiplier:.4f}")
        print(f"ğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
        print(f"   - Agentæ­¥æ•°: {agent_steps}")
        print(f"   - ç¯å¢ƒæ­¥æ•°: {env_steps}")
        print(f"   - å­¦ä¹ ç‡: {result.get('policy_learn_rate', 0):.6f}")
        print(f"   - ç†µ: {result.get('policy_entropy', 0):.4f}")
        print("=" * 80)
        
        # æ›´æ–°æœ€ä½³å¥–åŠ±
        if episode_reward_mean > self.best_reward:
            self.best_reward = episode_reward_mean
            self.best_iteration = iteration
            print(f"ğŸ‰ æ–°çš„æœ€ä½³å¥–åŠ±: {self.best_reward:.4f} (è¿­ä»£ {iteration})")
        
        # ä¿å­˜è®­ç»ƒç»“æœåˆ°æ–‡ä»¶
        self._save_training_result(result)
    
    def _save_training_result(self, result: Dict[str, Any]) -> None:
        """
        ä¿å­˜è®­ç»ƒç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            result: è®­ç»ƒç»“æœå­—å…¸
        """
        # åˆ›å»ºresultsç›®å½•
        results_dir = os.path.join(self.run_dir, "results")
        os.makedirs(results_dir, exist_ok=True)
        
        # ä¿å­˜å½“å‰è¿­ä»£ç»“æœ
        iteration = result.get("training_iteration", 0)
        result_file = os.path.join(results_dir, f"result_{iteration:06d}.json")
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    
    def on_checkpoint(self, checkpoint_info: Dict[str, Any]) -> None:
        """
        æ£€æŸ¥ç‚¹ä¿å­˜å›è°ƒ
        
        Args:
            checkpoint_info: æ£€æŸ¥ç‚¹ä¿¡æ¯å­—å…¸
        """
        checkpoint_path = checkpoint_info.get("checkpoint", "")
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def close(self) -> None:
        """å…³é—­å›è°ƒå‡½æ•°"""
        logger.info("ğŸ“Š è®­ç»ƒå®Œæˆ")
        logger.info(f"   æœ€ä½³å¥–åŠ±: {self.best_reward:.4f} (è¿­ä»£ {self.best_iteration})")
        logger.info(f"   æ€»è®­ç»ƒæ—¶é—´: {time.time() - self.start_time:.2f}ç§’")
        logger.removeHandler(self.file_handler)
        self.file_handler.close()


# ============================================================================
# å¥–åŠ±é‡å¡‘å‡½æ•°
# ============================================================================

def reward_shaping_with_lagrangian(
    batch: Dict[str, Any],
    lambda_: float,
    cost_limit: float
) -> Dict[str, Any]:
    """
    åŸºäºæ‹‰æ ¼æœ—æ—¥ä¹˜å­çš„å¥–åŠ±é‡å¡‘
    
    å°†çº¦æŸæˆæœ¬è½¬åŒ–ä¸ºå¥–åŠ±æƒ©ç½šï¼Œå®ç°çº¦æŸä¼˜åŒ–ã€‚
    
    å¥–åŠ±é‡å¡‘å…¬å¼ï¼š
        R' = R - Î» * (C - d)
    
    å…¶ä¸­ï¼š
        - R: åŸå§‹å¥–åŠ±
        - C: æˆæœ¬ï¼ˆå¹²é¢„æ¬¡æ•°ï¼‰
        - d: æˆæœ¬é™åˆ¶
        - Î»: æ‹‰æ ¼æœ—æ—¥ä¹˜å­
    
    Args:
        batch: æ ·æœ¬æ‰¹æ¬¡
        lambda_: æ‹‰æ ¼æœ—æ—¥ä¹˜å­
        cost_limit: æˆæœ¬é™åˆ¶
        
    Returns:
        batch: å¥–åŠ±é‡å¡‘åçš„æ‰¹æ¬¡
    """
    # æå–åŸå§‹å¥–åŠ±
    rewards = batch.get("rewards", np.zeros(len(batch)))
    
    # æå–æˆæœ¬ï¼ˆå¹²é¢„æ¬¡æ•°ï¼‰
    level1_interventions = batch.get("level1_interventions", np.zeros(len(batch)))
    level2_interventions = batch.get("level2_interventions", np.zeros(len(batch)))
    total_cost = level1_interventions + level2_interventions
    
    # è®¡ç®—çº¦æŸè¿å
    constraint_violation = total_cost - cost_limit
    
    # è®¡ç®—æ‹‰æ ¼æœ—æ—¥æƒ©ç½š
    lagrangian_penalty = lambda_ * constraint_violation
    
    # é‡å¡‘å¥–åŠ±
    shaped_rewards = rewards - lagrangian_penalty
    
    # æ›´æ–°æ‰¹æ¬¡
    batch["rewards"] = shaped_rewards
    batch["original_rewards"] = rewards
    batch["lagrangian_penalty"] = lagrangian_penalty
    
    return batch


# ============================================================================
# æ•°æ®éªŒè¯å‡½æ•°
# ============================================================================

def validate_batch(batch: Dict[str, Any]) -> bool:
    """
    éªŒè¯æ‰¹æ¬¡æ•°æ®çš„å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§
    
    Args:
        batch: æ ·æœ¬æ‰¹æ¬¡
        
    Returns:
        is_valid: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    """
    # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
    required_fields = [
        "obs", "actions", "rewards", "dones", "new_obs"
    ]
    
    for field in required_fields:
        if field not in batch:
            logger.error(f"âŒ æ‰¹æ¬¡ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            return False
    
    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    batch_size = len(batch["obs"])
    
    for field in ["obs", "actions", "rewards", "dones", "new_obs"]:
        if len(batch[field]) != batch_size:
            logger.error(f"âŒ å­—æ®µ {field} çš„é•¿åº¦ä¸åŒ¹é…: {len(batch[field])} != {batch_size}")
            return False
    
    # æ£€æŸ¥NaNå’ŒInf
    for field in ["obs", "actions", "rewards"]:
        data = batch[field]
        if isinstance(data, np.ndarray):
            if np.isnan(data).any():
                logger.error(f"âŒ å­—æ®µ {field} åŒ…å«NaNå€¼")
                return False
            if np.isinf(data).any():
                logger.error(f"âŒ å­—æ®µ {field} åŒ…å«Infå€¼")
                return False
    
    return True


# ============================================================================
# æ··åˆç²¾åº¦è®­ç»ƒé…ç½®
# ============================================================================

def configure_amp(config: PPOConfig, use_amp: bool) -> PPOConfig:
    """
    é…ç½®æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
    
    Args:
        config: PPOé…ç½®å¯¹è±¡
        use_amp: æ˜¯å¦å¯ç”¨AMP
        
    Returns:
        config: é…ç½®åçš„PPOé…ç½®
    """
    if use_amp:
        # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        config.training(
            use_amp=True,
            amp_dtype="float16"  # ä½¿ç”¨float16è¿›è¡ŒåŠ é€Ÿ
        )
        logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰å·²å¯ç”¨")
    else:
        config.training(use_amp=False)
        logger.info("â„¹ï¸  æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰å·²ç¦ç”¨")
    
    return config


# ============================================================================
# æ¢¯åº¦è£å‰ªé…ç½®
# ============================================================================

def configure_gradient_clipping(config: PPOConfig, grad_clip: float) -> PPOConfig:
    """
    é…ç½®æ¢¯åº¦è£å‰ª
    
    Args:
        config: PPOé…ç½®å¯¹è±¡
        grad_clip: æ¢¯åº¦è£å‰ªé˜ˆå€¼
        
    Returns:
        config: é…ç½®åçš„PPOé…ç½®
    """
    config.training(
        grad_clip=grad_clip
    )
    logger.info(f"âœ… æ¢¯åº¦è£å‰ªé˜ˆå€¼: {grad_clip}")
    
    return config


# ============================================================================
# Rayé…ç½®æ„å»º
# ============================================================================

def build_ray_config(user_config: Dict[str, Any]) -> PPOConfig:
    """
    æ„å»ºRay RLlibé…ç½®
    
    Args:
        user_config: ç”¨æˆ·é…ç½®å­—å…¸
        
    Returns:
        config: PPOConfigå¯¹è±¡
    """
    # åˆ›å»ºåŸºç¡€PPOé…ç½®
    config = PPOConfig()
    
    # ==================== ç¯å¢ƒé…ç½® ====================
    config.environment(
        env=SUMOGymEnv,
        env_config={
            "sumo_cfg_path": user_config["sumo_cfg_path"],
            "use_libsumo": user_config["use_libsumo"],
            "batch_subscribe": user_config["batch_subscribe"],
            "max_steps": user_config["max_steps"],
            "use_gui": user_config["use_gui"],
            "model_config": user_config["model"]["custom_model_config"]
        }
    )
    
    # ==================== æ¡†æ¶é…ç½® ====================
    config.framework(user_config["framework"])
    
    # ==================== å¹¶è¡Œé…ç½® ====================
    config.resources(
        num_gpus=user_config["num_gpus"],
        num_cpus_per_worker=user_config["num_cpus_per_worker"],
    )
    config.rollouts(
        num_rollout_workers=user_config["num_workers"],
        num_envs_per_worker=user_config["num_envs_per_worker"],
    )
    
    # ==================== è®­ç»ƒé…ç½® ====================
    config.training(
        train_batch_size=user_config["train_batch_size"],
        rollout_fragment_length=user_config["rollout_fragment_length"],
        sgd_minibatch_size=user_config["sgd_minibatch_size"],
        num_sgd_iter=user_config["num_sgd_iter"],
        lr=user_config["lr"],
        gamma=user_config["gamma"],
        lambda_=user_config["lambda_"],
        clip_param=user_config["clip_param"],
        vf_loss_coeff=user_config["vf_loss_coeff"],
        entropy_coeff=user_config["entropy_coeff"],
        kl_coeff=user_config["kl_coeff"],
        kl_target=user_config["kl_target"],
    )
    
    # ==================== æ¨¡å‹é…ç½® ====================
    config.model(
        custom_model=user_config["model"]["custom_model"],
        custom_model_config=user_config["model"]["custom_model_config"]
    )
    
    # ==================== æ¢¯åº¦è£å‰ªé…ç½® ====================
    config = configure_gradient_clipping(config, user_config["grad_clip"])
    
    # ==================== æ··åˆç²¾åº¦è®­ç»ƒé…ç½® ====================
    config = configure_amp(config, user_config["use_amp"])
    
    # ==================== æ£€æŸ¥ç‚¹é…ç½® ====================
    config.checkpointing(
        checkpoint_frequency=user_config["checkpoint_freq"],
        checkpoint_at_end=user_config["checkpoint_at_end"],
        checkpoint_score_attribute=user_config["checkpoint_score_attr"],
        keep_checkpoints_num=user_config["keep_checkpoints_num"],
    )
    
    # ==================== çº¦æŸä¼˜åŒ–é…ç½® ====================
    # å°†çº¦æŸä¼˜åŒ–å‚æ•°æ·»åŠ åˆ°é…ç½®ä¸­ï¼ˆä¾›ConstrainedPPOTrainerä½¿ç”¨ï¼‰
    config.cost_limit = user_config["cost_limit"]
    config.lambda_lr = user_config["lambda_lr"]
    config.lambda_init = user_config["lambda_init"]
    config.alpha = user_config["alpha"]
    config.beta = user_config["beta"]
    
    # ==================== æ—¥å¿—é…ç½® ====================
    config.logging(
        level=user_config["log_level"],
    )
    
    logger.info("âœ… Ray RLlibé…ç½®æ„å»ºå®Œæˆ")
    
    return config


# ============================================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================================================

def train(config: Dict[str, Any]) -> None:
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        config: è®­ç»ƒé…ç½®å­—å…¸
    """
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸš€ Ray RLlib è®­ç»ƒé…ç½®")
    print("=" * 80)
    print(f"ğŸ“Š ç¯å¢ƒé…ç½®:")
    print(f"   - SUMOé…ç½®æ–‡ä»¶: {config['sumo_cfg_path']}")
    print(f"   - LIBSUMO_AS_TRACI: {config['use_libsumo']}")
    print(f"   - æ‰¹é‡è®¢é˜…: {config['batch_subscribe']}")
    print(f"   - æœ€å¤§æ­¥æ•°: {config['max_steps']}")
    print(f"ğŸ–¥ï¸  è®¡ç®—èµ„æºé…ç½®:")
    print(f"   - Workersæ•°é‡: {config['num_workers']}")
    print(f"   - GPUæ•°é‡: {config['num_gpus']}")
    print(f"   - æ¯Worker CPUæ•°: {config['num_cpus_per_worker']}")
    print(f"   - æ¯Workerç¯å¢ƒæ•°: {config['num_envs_per_worker']}")
    print(f"ğŸ“ˆ è®­ç»ƒé…ç½®:")
    print(f"   - è®­ç»ƒæ‰¹æ¬¡å¤§å°: {config['train_batch_size']}")
    print(f"   - Rolloutç‰‡æ®µé•¿åº¦: {config['rollout_fragment_length']}")
    print(f"   - SGDå°æ‰¹æ¬¡å¤§å°: {config['sgd_minibatch_size']}")
    print(f"   - SGDè¿­ä»£æ¬¡æ•°: {config['num_sgd_iter']}")
    print(f"   - å­¦ä¹ ç‡: {config['lr']}")
    print(f"ğŸ” çº¦æŸä¼˜åŒ–é…ç½®:")
    print(f"   - æˆæœ¬é™åˆ¶: {config['cost_limit']}")
    print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­å­¦ä¹ ç‡: {config['lambda_lr']}")
    print(f"   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­åˆå§‹å€¼: {config['lambda_init']}")
    print(f"ğŸ›¡ï¸  ä¼˜åŒ–é…ç½®:")
    print(f"   - æ¢¯åº¦è£å‰ª: {config['grad_clip']}")
    print(f"   - æ··åˆç²¾åº¦è®­ç»ƒ: {config['use_amp']}")
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹é…ç½®:")
    print(f"   - ä¿å­˜é¢‘ç‡: {config['checkpoint_freq']}")
    print(f"   - ä¿ç•™æ•°é‡: {config['keep_checkpoints_num']}")
    print("=" * 80)
    
    # åˆå§‹åŒ–Ray
    if not ray.is_initialized():
        ray.init(
            num_gpus=config["num_gpus"],
            num_cpus=config["num_workers"] * config["num_cpus_per_worker"] + 2,
            log_to_driver=config.get("log_level", "INFO") == "INFO"
        )
        logger.info("âœ… Rayå·²åˆå§‹åŒ–")
    
    # æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹
    register_traffic_controller_model()
    
    # æ³¨å†Œç¯å¢ƒ
    tune.register_env("sumo_gym_env", env_creator)
    
    # æ„å»ºRayé…ç½®
    ray_config = build_ray_config(config)
    
    # åˆ›å»ºè®­ç»ƒå›è°ƒ
    callback = TrainingCallback(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    if config.get("restore"):
        logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: {config['restore']}")
        trainer = ConstrainedPPOTrainer(
            config=ray_config.to_dict(),
            logger_creator=lambda config: None
        )
        trainer.restore(config["restore"])
    else:
        trainer = ConstrainedPPOTrainer(
            config=ray_config.to_dict(),
            logger_creator=lambda config: None
        )
    
    logger.info("âœ… è®­ç»ƒå™¨å·²åˆ›å»º")
    
    # è®­ç»ƒå¾ªç¯
    stop_criteria = config["stop"]
    max_iterations = stop_criteria.get("training_iteration", 1000)
    target_reward = stop_criteria.get("episode_reward_mean", None)
    
    print("\n" + "=" * 80)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
    print("=" * 80)
    
    try:
        for iteration in range(max_iterations):
            # è®­ç»ƒä¸€ä¸ªè¿­ä»£
            result = trainer.train()
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            callback.on_train_result(result)
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if target_reward is not None:
                current_reward = result.get("episode_reward_mean", 0.0)
                if current_reward >= target_reward:
                    print(f"\nğŸ‰ è¾¾åˆ°ç›®æ ‡å¥–åŠ±: {current_reward:.4f} >= {target_reward:.4f}")
                    break
            
            # æ£€æŸ¥ç‚¹ä¿å­˜
            if (iteration + 1) % config["checkpoint_freq"] == 0:
                checkpoint_path = trainer.save()
                callback.on_checkpoint({"checkpoint": checkpoint_path})
    
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    
    finally:
        # è®­ç»ƒç»“æŸæ—¶ä¿å­˜æ£€æŸ¥ç‚¹
        if config["checkpoint_at_end"]:
            checkpoint_path = trainer.save()
            logger.info(f"ğŸ’¾ æœ€ç»ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # å…³é—­å›è°ƒ
        callback.close()
        
        # å…³é—­è®­ç»ƒå™¨
        trainer.stop()
        
        # å…³é—­Ray
        ray.shutdown()
        logger.info("âœ… Rayå·²å…³é—­")


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def parse_args() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="Ray RLlib è®­ç»ƒè„šæœ¬ - SUMOäº¤é€šæ§åˆ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
    python ray_train.py
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    python ray_train.py --config config.json
    
    # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    python ray_train.py --restore /path/to/checkpoint
    
    # è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
    python ray_train.py --num_workers 8 --num_gpus 2 --train_batch_size 8000
        """
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰"
    )
    
    parser.add_argument(
        "--restore",
        type=str,
        default=None,
        help="æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œç”¨äºæ¢å¤è®­ç»ƒ"
    )
    
    parser.add_argument(
        "--num_workers",
        type=int,
        default=None,
        help="RolloutWorkersæ•°é‡"
    )
    
    parser.add_argument(
        "--num_gpus",
        type=int,
        default=None,
        help="GPUæ•°é‡"
    )
    
    parser.add_argument(
        "--train_batch_size",
        type=int,
        default=None,
        help="è®­ç»ƒæ‰¹æ¬¡å¤§å°"
    )
    
    parser.add_argument(
        "--rollout_fragment_length",
        type=int,
        default=None,
        help="Rolloutç‰‡æ®µé•¿åº¦"
    )
    
    parser.add_argument(
        "--max_iterations",
        type=int,
        default=None,
        help="æœ€å¤§è®­ç»ƒè¿­ä»£æ¬¡æ•°"
    )
    
    parser.add_argument(
        "--log_dir",
        type=str,
        default=None,
        help="æ—¥å¿—ç›®å½•"
    )
    
    parser.add_argument(
        "--experiment_name",
        type=str,
        default=None,
        help="å®éªŒåç§°"
    )
    
    parser.add_argument(
        "--use_libsumo",
        action="store_true",
        help="å¯ç”¨LIBSUMO_AS_TRACIåŠ é€Ÿ"
    )
    
    parser.add_argument(
        "--use_gui",
        action="store_true",
        help="å¯ç”¨SUMO GUI"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # è·å–é»˜è®¤é…ç½®
    config = get_default_config()
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    if args.config:
        user_config = load_config_from_file(args.config)
        config = merge_configs(config, user_config)
    
    # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
    if args.num_workers is not None:
        config["num_workers"] = args.num_workers
    if args.num_gpus is not None:
        config["num_gpus"] = args.num_gpus
    if args.train_batch_size is not None:
        config["train_batch_size"] = args.train_batch_size
    if args.rollout_fragment_length is not None:
        config["rollout_fragment_length"] = args.rollout_fragment_length
    if args.max_iterations is not None:
        config["stop"]["training_iteration"] = args.max_iterations
    if args.log_dir is not None:
        config["log_dir"] = args.log_dir
    if args.experiment_name is not None:
        config["experiment_name"] = args.experiment_name
    if args.use_libsumo:
        config["use_libsumo"] = True
    if args.use_gui:
        config["use_gui"] = True
    
    # è®¾ç½®æ¢å¤è·¯å¾„
    if args.restore:
        config["restore"] = args.restore
    
    # å¼€å§‹è®­ç»ƒ
    train(config)


# ============================================================================
# è„šæœ¬å…¥å£
# ============================================================================

if __name__ == "__main__":
    main()
