"""
é¢„æµ‹å±‚ï¼šæ¸è¿›å¼ä¸–ç•Œæ¨¡å‹ (Progressive World Model)
åˆ†ä¸¤é˜¶æ®µè®­ç»ƒï¼š
Phase 1ï¼šä»…é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»è½¦è¾†çŠ¶æ€ï¼ˆä½ç½®ã€é€Ÿåº¦ï¼‰ï¼Œå­¦ä¹ åŸºç¡€åŠ¨åŠ›å­¦
Phase 2ï¼šå†»ç»“ç‰¹å¾æå–å™¨ï¼Œè§£è€¦è¾“å‡ºä¸º z_flowï¼ˆæµæ¼”åŒ–ï¼‰ä¸ z_riskï¼ˆé£é™©æ¼”åŒ–ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any, Optional


class ProgressiveWorldModel(nn.Module):
    """
    æ¸è¿›å¼ä¸–ç•Œæ¨¡å‹
    é˜¶æ®µ1ï¼šé¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çŠ¶æ€
    é˜¶æ®µ2ï¼šé¢„æµ‹æœªæ¥5æ­¥çŠ¶æ€ + å†²çªæ¦‚ç‡
    """
    
    def __init__(self, input_dim: int = 256, hidden_dim: int = 128, 
                 future_steps: int = 5, num_phases: int = 2):
        super().__init__()
        
        self.future_steps = future_steps
        self.num_phases = num_phases
        self.current_phase = 1
        
        # 1. å…±äº«ç¼–ç å™¨
        self.shared_encoder = nn.Sequential(
            nn.Linear(input_dim, 192),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(192, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # 2. åŸºç¡€åŠ¨åŠ›å­¦åˆ†æ”¯ (Phase 1)
        self.dynamics_lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1
        )
        
        # 3. é£é™©æ¼”åŒ–åˆ†æ”¯ (Phase 2)
        self.risk_decoders = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, 192),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(192, input_dim + 1)  # çŠ¶æ€ + å†²çªæ¦‚ç‡
            ) for _ in range(future_steps)
        ])
        
        # 4. è¾…åŠ©åˆ†ç±»å™¨
        self.conflict_classifier = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, (nn.Linear, nn.LSTM)):
                if hasattr(m, 'weight') and m.weight is not None:
                    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def set_phase(self, phase: int):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        self.current_phase = phase
        print(f"ğŸ”„ ä¸–ç•Œæ¨¡å‹åˆ‡æ¢åˆ°é˜¶æ®µ {phase}")
    
    def forward(self, gnn_embedding: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            gnn_embedding: [N, 256] GNNè¾“å‡ºåµŒå…¥
        Returns:
            predictions: 
                Phase 1: [N, 256] ä¸‹ä¸€æ—¶åˆ»çŠ¶æ€
                Phase 2: [N, 5, 257] æœªæ¥5æ­¥çŠ¶æ€ + å†²çªæ¦‚ç‡
        """
        batch_size = gnn_embedding.size(0)
        
        # 1. å…±äº«ç¼–ç 
        encoded = self.shared_encoder(gnn_embedding)  # [N, 128]
        
        if self.current_phase == 1:
            # Phase 1: åŸºç¡€åŠ¨åŠ›å­¦é¢„æµ‹
            # é‡å¡‘ä¸ºLSTMè¾“å…¥æ ¼å¼ [N, 1, 128]
            lstm_input = encoded.unsqueeze(1)
            
            # LSTMé¢„æµ‹
            lstm_output, _ = self.dynamics_lstm(lstm_input)  # [N, 1, 128]
            
            # é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çŠ¶æ€
            next_state = self.risk_decoders[0](lstm_output.squeeze(1))[:, :-1]  # [N, 256]
            
            return next_state
        
        else:
            # Phase 2: é£é™©æ¼”åŒ–é¢„æµ‹
            predictions = []
            
            # ä¸ºæ¯ä¸ªæœªæ¥æ­¥ç”Ÿæˆé¢„æµ‹
            for t in range(self.future_steps):
                # ä½¿ç”¨ç›¸åŒçš„ç¼–ç ä½†æ·»åŠ æ—¶é—´æ­¥ä¿¡æ¯
                time_input = encoded + 0.1 * t * torch.ones_like(encoded)
                pred = self.risk_decoders[t](time_input)  # [N, 257]
                predictions.append(pred.unsqueeze(1))
            
            # åˆå¹¶é¢„æµ‹ [N, 5, 257]
            predictions = torch.cat(predictions, dim=1)
            
            return predictions
    
    def compute_loss(self, predictions: torch.Tensor, targets: torch.Tensor, 
                     phase: int = None) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æŸå¤±
        Args:
            predictions: æ¨¡å‹é¢„æµ‹
            targets: çœŸå®ç›®æ ‡
            phase: å½“å‰è®­ç»ƒé˜¶æ®µ
        Returns:
            loss_dict: åŒ…å«å„é¡¹æŸå¤±çš„å­—å…¸
        """
        if phase is None:
            phase = self.current_phase
        
        loss_dict = {}
        
        if phase == 1:
            # Phase 1: ä»…è®¡ç®—çŠ¶æ€é¢„æµ‹çš„MSEæŸå¤±
            mse_loss = F.mse_loss(predictions, targets)
            loss_dict['mse_loss'] = mse_loss
            loss_dict['total_loss'] = mse_loss
        
        else:
            # Phase 2: è”åˆä¼˜åŒ–è½¨è¿¹MSEä¸å†²çªåˆ†ç±»æŸå¤±
            # çŠ¶æ€é¢„æµ‹æŸå¤±
            state_pred = predictions[:, :, :-1]  # [N, 5, 256]
            state_target = targets[:, :, :-1]  # [N, 5, 256]
            mse_loss = F.mse_loss(state_pred, state_target)
            
            # å†²çªåˆ†ç±»æŸå¤±
            conflict_pred = predictions[:, :, -1]  # [N, 5]
            conflict_target = targets[:, :, -1]  # [N, 5]
            bce_loss = F.binary_cross_entropy(conflict_pred, conflict_target)
            
            # æ€»æŸå¤±
            total_loss = mse_loss + 0.5 * bce_loss
            
            loss_dict['mse_loss'] = mse_loss
            loss_dict['bce_loss'] = bce_loss
            loss_dict['total_loss'] = total_loss
        
        return loss_dict


class FlowEvolutionDecoder(nn.Module):
    """
    æµæ¼”åŒ–è§£ç å™¨
    é¢„æµ‹äº¤é€šæµçš„æ¼”åŒ–è¶‹åŠ¿
    """
    
    def __init__(self, hidden_dim: int = 128, output_dim: int = 256):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, 192),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(192, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
    
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            hidden_state: [N, hidden_dim] éšè—çŠ¶æ€
        Returns:
            flow_state: [N, output_dim] æµçŠ¶æ€
        """
        return self.decoder(hidden_state)


class RiskEvolutionDecoder(nn.Module):
    """
    é£é™©æ¼”åŒ–è§£ç å™¨
    é¢„æµ‹é£é™©çŠ¶æ€çš„æ¼”åŒ–
    """
    
    def __init__(self, hidden_dim: int = 128, output_dim: int = 256):
        super().__init__()
        
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, 192),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(192, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
        
        # é£é™©åˆ†ç±»å¤´
        self.risk_classifier = nn.Sequential(
            nn.Linear(output_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    
    def forward(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            hidden_state: [N, hidden_dim] éšè—çŠ¶æ€
        Returns:
            risk_state: [N, output_dim] é£é™©çŠ¶æ€
            risk_prob: [N, 1] é£é™©æ¦‚ç‡
        """
        risk_state = self.decoder(hidden_state)
        risk_prob = self.risk_classifier(risk_state)
        return risk_state, risk_prob
