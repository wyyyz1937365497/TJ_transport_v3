"""
åŸºäºSUMO-RLæ¡†æ¶çš„è®­ç»ƒè„šæœ¬
ä½¿ç”¨SUMOä»¿çœŸç¯å¢ƒè®­ç»ƒç¥ç»ç½‘ç»œæ§åˆ¶å™¨
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Any, Optional
import os
import json
from tqdm import tqdm

from neural_traffic_controller import TrafficController
from sumo_rl_env import SUMORLEnvironment, create_sumo_env


class SUMORLTrainer:
    """
    åŸºäºSUMO-RLçš„è®­ç»ƒå™¨
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºSUMOç¯å¢ƒ
        self.env = create_sumo_env(
            sumo_cfg_path=config['sumo_cfg_path'],
            use_gui=config.get('use_gui', False),
            max_steps=config.get('max_steps', 3600),
            seed=config.get('seed', None)
        )
        
        # åˆ›å»ºç¥ç»ç½‘ç»œæ§åˆ¶å™¨
        self.controller = TrafficController(config['model']).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.controller.parameters(),
            lr=config.get('learning_rate', 1e-4),
            weight_decay=config.get('weight_decay', 1e-5)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=0.5,
            patience=10,
            verbose=True
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_stats = []
        self.best_reward = float('-inf')
        
        # æ¨¡å‹ä¿å­˜è·¯å¾„
        self.save_dir = config.get('save_dir', 'models')
        os.makedirs(self.save_dir, exist_ok=True)
        
        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ!")
    
    def build_batch(self, observation: Dict[str, Any], step: int) -> Dict[str, Any]:
        """
        æ„å»ºè®­ç»ƒæ‰¹æ¬¡
        
        Args:
            observation: ç¯å¢ƒè§‚æµ‹
            step: å½“å‰æ­¥æ•°
        
        Returns:
            batch: è®­ç»ƒæ‰¹æ¬¡
        """
        vehicle_data = observation['vehicle_data']
        vehicle_ids = observation['vehicle_ids']
        
        if not vehicle_data:
            return None
        
        # 1. æ”¶é›†è½¦è¾†ç‰¹å¾
        node_features = []
        is_icv_list = []
        
        for veh_id in vehicle_ids:
            vehicle = vehicle_data[veh_id]
            
            # èŠ‚ç‚¹ç‰¹å¾: [ä½ç½®, é€Ÿåº¦, åŠ é€Ÿåº¦, è½¦é“, å‰©ä½™è·ç¦», å®Œæˆç‡, ç±»å‹, æ—¶é—´, æ­¥é•¿]
            features = [
                vehicle.get('position', 0.0),
                vehicle.get('speed', 0.0),
                vehicle.get('acceleration', 0.0),
                vehicle.get('lane_index', 0),
                1000.0,  # å‰©ä½™è·ç¦»ï¼ˆç®€åŒ–ï¼‰
                0.5,  # å®Œæˆç‡ï¼ˆç®€åŒ–ï¼‰
                1.0 if vehicle.get('is_icv', False) else 0.0,
                step * 0.1,
                0.1
            ]
            
            node_features.append(features)
            is_icv_list.append(vehicle.get('is_icv', False))
        
        # 2. æ„å»ºäº¤äº’å›¾
        edge_indices = []
        edge_features = []
        
        # ç®€åŒ–ç‰ˆï¼šè¿æ¥ç›¸è¿‘è½¦è¾†
        for i, veh_id_i in enumerate(vehicle_ids):
            for j, veh_id_j in enumerate(vehicle_ids):
                if i == j:
                    continue
                
                pos_i = vehicle_data[veh_id_i].get('position', 0.0)
                pos_j = vehicle_data[veh_id_j].get('position', 0.0)
                speed_i = vehicle_data[veh_id_i].get('speed', 0.0)
                speed_j = vehicle_data[veh_id_j].get('speed', 0.0)
                
                distance = abs(pos_i - pos_j)
                if distance < 50:  # 50ç±³å†…
                    edge_indices.append([i, j])
                    
                    rel_distance = distance
                    rel_speed = abs(speed_i - speed_j)
                    
                    ttc = rel_distance / max(rel_speed, 0.1) if rel_speed > 0 else 100
                    thw = rel_distance / max(speed_i, 0.1) if speed_i > 0 else 100
                    
                    edge_features.append([rel_distance, rel_speed, min(ttc, 10), min(thw, 10)])
        
        # 3. è½¬æ¢ä¸ºå¼ é‡
        batch = {
            'node_features': torch.tensor(node_features, dtype=torch.float32).to(self.device),
            'edge_indices': torch.tensor(edge_indices, dtype=torch.long).t().contiguous().to(self.device) if edge_indices else torch.zeros((2, 0), dtype=torch.long).to(self.device),
            'edge_features': torch.tensor(edge_features, dtype=torch.float32).to(self.device) if edge_features else torch.zeros((0, 4), dtype=torch.float32).to(self.device),
            'global_metrics': torch.tensor(observation['global_metrics'], dtype=torch.float32).unsqueeze(0).to(self.device),
            'vehicle_ids': vehicle_ids,
            'is_icv': torch.tensor(is_icv_list, dtype=torch.bool).to(self.device),
            'vehicle_states': {
                'ids': vehicle_ids,
                'data': vehicle_data
            }
        }
        
        return batch
    
    def run_episode(self, episode_num: int) -> Dict[str, float]:
        """
        è¿è¡Œä¸€ä¸ªepisode
        
        Args:
            episode_num: episodeç¼–å·
        
        Returns:
            stats: episodeç»Ÿè®¡ä¿¡æ¯
        """
        # é‡ç½®ç¯å¢ƒ
        observation = self.env.reset()
        
        episode_reward = 0.0
        step = 0
        
        # è®¾ç½®ä¸–ç•Œæ¨¡å‹é˜¶æ®µ
        phase = self.config.get('training_phase', 1)
        self.controller.world_model.set_phase(phase)
        
        # Episodeå¾ªç¯
        while step < self.env.max_steps:
            # æ„å»ºæ‰¹æ¬¡
            batch = self.build_batch(observation, step)
            
            if batch is None:
                # æ²¡æœ‰è½¦è¾†ï¼Œç›´æ¥æ‰§è¡Œä¸€æ­¥
                observation, reward, done, info = self.env.step({})
                episode_reward += reward
                step += 1
                continue
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                output = self.controller(batch, step)
            
            # æ„å»ºåŠ¨ä½œ
            action = {
                'selected_vehicle_ids': output['selected_vehicle_ids'],
                'safe_actions': output['safe_actions']
            }
            
            # æ‰§è¡Œä¸€æ­¥
            observation, reward, done, info = self.env.step(action)
            episode_reward += reward
            step += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if step % 100 == 0:
                print(f"[Episode {episode_num}] Step {step}/{self.env.max_steps}, "
                      f"Reward: {episode_reward:.2f}, "
                      f"Vehicles: {info['vehicle_count']}")
            
            if done:
                break
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        env_stats = self.env.get_episode_statistics()
        
        stats = {
            'episode': episode_num,
            'total_reward': episode_reward,
            'total_steps': step,
            'avg_reward': episode_reward / max(step, 1),
            'vehicle_count': env_stats['vehicle_count']
        }
        
        return stats
    
    def train(self, num_episodes: int = 100):
        """
        è®­ç»ƒä¸»å¾ªç¯
        
        Args:
            num_episodes: è®­ç»ƒepisodeæ•°
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"æ€»episodes: {num_episodes}")
        print(f"æœ€å¤§æ­¥æ•°: {self.env.max_steps}")
        print(f"å­¦ä¹ ç‡: {self.config.get('learning_rate', 1e-4)}")
        print(f"è®­ç»ƒé˜¶æ®µ: {self.config.get('training_phase', 1)}")
        print(f"{'='*60}\n")
        
        for episode in tqdm(range(1, num_episodes + 1), desc="Training"):
            # è¿è¡Œepisode
            stats = self.run_episode(episode)
            
            # è®°å½•ç»Ÿè®¡
            self.episode_rewards.append(stats['total_reward'])
            self.episode_stats.append(stats)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(stats['avg_reward'])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if stats['avg_reward'] > self.best_reward:
                self.best_reward = stats['avg_reward']
                self.save_model('best_model.pth')
                print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {self.best_reward:.2f}")
            
            # å®šæœŸä¿å­˜
            if episode % 10 == 0:
                self.save_model(f'checkpoint_episode_{episode}.pth')
                self.save_training_log()
            
            # æ‰“å°ç»Ÿè®¡
            print(f"\nEpisode {episode} å®Œæˆ:")
            print(f"  æ€»å¥–åŠ±: {stats['total_reward']:.2f}")
            print(f"  å¹³å‡å¥–åŠ±: {stats['avg_reward']:.4f}")
            print(f"  æ€»æ­¥æ•°: {stats['total_steps']}")
            print(f"  è½¦è¾†æ•°: {stats['vehicle_count']}")
            print(f"  æœ€ä½³å¹³å‡å¥–åŠ±: {self.best_reward:.4f}\n")
        
        print(f"\n{'='*60}")
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"{'='*60}")
        print(f"æœ€ç»ˆæœ€ä½³å¹³å‡å¥–åŠ±: {self.best_reward:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model('final_model.pth')
        self.save_training_log()
    
    def save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.save_dir, filename)
        torch.save({
            'episode': len(self.episode_rewards),
            'model_state_dict': self.controller.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_reward': self.best_reward,
            'config': self.config
        }, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def save_training_log(self):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_path = os.path.join(self.save_dir, 'training_log.json')
        
        log_data = {
            'config': self.config,
            'episode_rewards': self.episode_rewards,
            'episode_stats': self.episode_stats,
            'best_reward': float(self.best_reward)
        }
        
        with open(log_path, 'w') as f:
            json.dump(log_data, f, indent=2)
        
        print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}")


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = json.load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤é…ç½®
    default_config = {
        'sumo_cfg_path': 'ä»¿çœŸç¯å¢ƒ-åˆèµ›/sumo.sumocfg',
        'use_gui': False,
        'max_steps': 3600,
        'seed': 42,
        
        'model': {
            'node_dim': 9,
            'edge_dim': 4,
            'gnn_hidden_dim': 64,
            'gnn_output_dim': 256,
            'gnn_layers': 3,
            'gnn_heads': 4,
            'world_hidden_dim': 128,
            'future_steps': 5,
            'controller_hidden_dim': 128,
            'global_dim': 16,
            'top_k': 5,
            'ttc_threshold': 2.0,
            'thw_threshold': 1.5,
            'max_accel': 2.0,
            'max_decel': -3.0,
            'emergency_decel': -5.0,
            'max_lane_change_speed': 5.0,
            'cost_limit': 0.1,
            'lambda_lr': 0.01,
            'cache_timeout': 10,
            'device': 'cpu'
        },
        
        'learning_rate': 1e-4,
        'weight_decay': 1e-5,
        'num_episodes': 100,
        'training_phase': 1,
        'save_dir': 'models'
    }
    
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    config_path = 'train_sumo_rl_config.json'
    if os.path.exists(config_path):
        config = load_config(config_path)
        # åˆå¹¶é»˜è®¤é…ç½®
        for key, value in default_config.items():
            if key not in config:
                config[key] = value
    else:
        config = default_config
        # ä¿å­˜é»˜è®¤é…ç½®
        with open(config_path, 'w') as f:
            json.dump(default_config, f, indent=2)
        print(f"ğŸ“ é»˜è®¤é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SUMORLTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(num_episodes=config['num_episodes'])


if __name__ == "__main__":
    main()
